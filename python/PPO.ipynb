{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is loading the 1th model with the following values:\n",
      "Loading mode to train from Projects/Steering/v8.0cm/m1/Models\n",
      "{'batch_size': 128,\n",
      " 'beta': 0.001,\n",
      " 'buffer_size': 4096,\n",
      " 'curriculum_file': 'Projects/Steering/Builds/v5.0c.json',\n",
      " 'env_name': 'v8.1cm',\n",
      " 'epsilon': 0.2,\n",
      " 'gamma': 0.99,\n",
      " 'hidden_units': 512,\n",
      " 'lambd': 0.95,\n",
      " 'leaning_rate': 5e-05,\n",
      " 'max_steps': 50000000000.0,\n",
      " 'num_epoch': 5,\n",
      " 'projet_name': 'Steering',\n",
      " 'time_horizon': 2048}\n"
     ]
    }
   ],
   "source": [
    "#Change these two names when you work on a new project\n",
    "project_name = \"Steering\" # The sub-directory name for model and summary statistics or work on a new build \n",
    "env_name =     \"v8.1cm\" # Name of the training environment file.\n",
    "json_name = \"v5.0c\"\n",
    "\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "\n",
    "load_model_from_diff_env = True\n",
    "model_number_to_load = 0 # which model to load\n",
    "                         #If this value is 0, load the last model in the given version.\n",
    "                         #If this value is -1, then load the last model of another version\n",
    "load_model_env_name = \"v8.0cm\"\n",
    "    \n",
    "load_best_model = False # You can either load the checkpoint with the best value or load the last checkpoint.\n",
    "\n",
    "max_steps = 5e10 # Set maximum number of steps to run environment.\n",
    "summary_freq = 2500 # Frequency at which to save training statistics.\n",
    "save_freq = 25000 # Frequency at which to save model.\n",
    "\n",
    "\n",
    "\n",
    "### Setting up the necesary directory structure and naming conventions\n",
    "project_path = \"Projects/{}/Builds/{}\".format(project_name, env_name)\n",
    "curiculum_path = \"Projects/{}/Builds/{}.json\".format(project_name, json_name)\n",
    "\n",
    "curriculum_file = curiculum_path\n",
    "\n",
    "env_path = \"Projects/{}/{}\".format(project_name, env_name)\n",
    "load_model_env_path = \"Projects/{}/{}\".format(project_name, load_model_env_name)\n",
    "\n",
    "if not os.path.exists(env_path):\n",
    "    os.makedirs(env_path)\n",
    "\n",
    "\n",
    "if not load_model:\n",
    "    model_count = len([i for i in os.listdir(env_path) if os.path.isdir(env_path)]) + 1\n",
    "    print(\"This is the {}th model\".format(model_count))\n",
    "elif load_model:\n",
    "    if model_number_to_load == 0:\n",
    "        model_number_to_load =  len([i for i in os.listdir(env_path) if os.path.isdir(env_path)])\n",
    "    elif load_model_from_diff_env and model_number_to_load == 0:\n",
    "        model_number_to_load =  len([i for i in os.listdir(load_model_env_path) if os.path.isdir(load_model_env_path)])\n",
    "        \n",
    "    print(\"This is loading the {}th model with the following values:\".format(model_number_to_load))\n",
    "    \n",
    "    model_count = model_number_to_load\n",
    "\n",
    "env_summary_path = 'Projects/{}/{}/m{}/Summaries'.format(project_name, env_name, model_count)\n",
    "if load_model_from_diff_env: \n",
    "    load_model_env_path = 'Projects/{}/{}/m{}/Models'.format(project_name, load_model_env_name, model_count)\n",
    "    print(\"Loading mode to train from {}\".format(load_model_env_path))\n",
    "env_model_path = 'Projects/{}/{}/m{}/Models'.format(project_name, env_name, model_count)\n",
    "\n",
    "if not os.path.exists(env_model_path):\n",
    "    os.makedirs(env_model_path)\n",
    "\n",
    "if not os.path.exists(env_summary_path):\n",
    "    os.makedirs(env_summary_path)\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 3 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 * 2 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 5e-5 # Model learning rate.\n",
    "hidden_units = 512 # Number of units in hidden layer.\n",
    "batch_size = 64 * 2 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'projet_name':project_name, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffer_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}\n",
    "\n",
    "documentation = \\\n",
    "'''\n",
    "V7\n",
    "No rotation, static obstacles. 5 wall seekers\n",
    "No noop\n",
    "NUM ENV = 1\n",
    "\n",
    "STATES:\n",
    "\n",
    "        List<float> state = new List<float>();\n",
    "\n",
    "        state.Add(transform.rotation.eulerAngles.y / 180.0f - 1.0f);\n",
    "\n",
    "        state.Add(car_rb.velocity.x);\n",
    "        state.Add(car_rb.velocity.z);\n",
    "\n",
    "        state.Add(car_rb.angularVelocity.y);\n",
    "\n",
    "        state.Add(Vector3.Distance(transform.position, target.transform.position));\n",
    "        state.Add(target.transform.position.x - transform.position.x);\n",
    "        state.Add(target.transform.position.z - transform.position.z);\n",
    "\n",
    "        Vector3[]  wall_points = FindClosestTargetPoints(\"Wall\", 5);\n",
    "        foreach (Vector3 wall_point in wall_points) {\n",
    "            if (DebugMode) Debug.DrawLine(transform.position, wall_point);\n",
    "            state.Add(Vector3.Distance(transform.position, wall_point));\n",
    "            state.Add(wall_point.x - transform.position.x);\n",
    "            state.Add(wall_point.z - transform.position.z);\n",
    "        }\n",
    "\n",
    "        \n",
    "    private void OnCollisionEnter(Collision collision) {\n",
    "\n",
    "COLLISION:\n",
    "        if(rewardStyle == RewardStyle.ProgressAvoidObs) {\n",
    "            if (collision.collider.gameObject.tag == \"Wall\" || collision.collider.gameObject.tag == \"Obstacle\") {\n",
    "                reward -= 1f;\n",
    "                cumulReward = +1;\n",
    "                done = true;\n",
    "                //Debug.Log(string.Format(\"OnCollisionEnter cummulative reward is:{0}\", CumulativeReward));\n",
    "            }\n",
    "        }\n",
    "        \n",
    "AGENT STEP:\n",
    "        action = (int)act[0];\n",
    "        if (action == 0) {\n",
    "            car_rb.AddForce(transform.forward * speed_mult, ForceMode.VelocityChange);\n",
    "        }\n",
    "        if (action == 1) {\n",
    "            car_rb.AddTorque(0f, turn_mult, 0f, ForceMode.VelocityChange);\n",
    "        }\n",
    "        if (action == 2) {\n",
    "            car_rb.AddTorque(0f, -turn_mult, 0f, ForceMode.VelocityChange);\n",
    "        }\n",
    "        car_rb.velocity = car_rb.velocity.magnitude * transform.forward;\n",
    "\n",
    "        if (car_rb.velocity.magnitude > maxSpeed) car_rb.velocity = car_rb.velocity * slowing_down_constant;\n",
    "\n",
    "            case RewardStyle.ProgressAvoidObs:\n",
    "                hitting_target_rewards_one = true;\n",
    "\n",
    "                this_distance = Vector3.Distance(transform.position, Target.transform.position);\n",
    "                delta = last_distance - this_distance;\n",
    "                last_distance = this_distance;\n",
    "\n",
    "                reward += delta / 10f;\n",
    "                reward -= MinusRewardStep;\n",
    "                break;\n",
    "\n",
    "        }\n",
    "        }\n",
    "'''\n",
    "if True:\n",
    "    pprint(hyperparameter_dict)\n",
    "    \n",
    "#Saving the model details in the summary.\n",
    "model_name = \"{}_m{}\".format(env_name, model_count)    \n",
    "with open('./Projects/{}/{}/m{}/Summaries/{}.txt'.format(project_name, env_name, model_count, model_name), 'w') as file:\n",
    "    file.write(\"gamma = {}\\n\".format(gamma))\n",
    "    file.write(\"lambd = {}\\n\".format(lambd))\n",
    "    file.write(\"time_horizon = {}\\n\".format(time_horizon))\n",
    "    file.write(\"beta = {}\\n\".format(beta))\n",
    "    file.write(\"num_epoch = {}\\n\".format(num_epoch))\n",
    "    file.write(\"epsilon = {}\\n\".format(epsilon))\n",
    "    file.write(\"buffer_size = {}\\n\".format(buffer_size))\n",
    "    file.write(\"learning_rate = {}\\n\".format(learning_rate))\n",
    "    file.write(\"hidden_units = {}\\n\".format(hidden_units))\n",
    "    file.write(\"batch_size = {}\\n\\n\".format(batch_size))\n",
    "    file.write(\"documentation = {}\\n\".format(documentation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects/Steering/Builds/v8.1cm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\ttarget_position_z -> 0.0\n",
      "\t\tnum_obstacles -> 1.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 22\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 3\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: Forward, Right, Left\n"
     ]
    }
   ],
   "source": [
    "# When the environment crashes it take a while for the socket to get freed up. This random port selection within a range\n",
    "# helps with that.\n",
    "port_save = random.randint(0,100)\n",
    "print(project_path)\n",
    "env = UnityEnvironment(file_name=project_path, curriculum=curriculum_file, base_port = 6083 + port_save)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started for v8.1cm\n",
      "Loading Model from different env with path Projects/Steering/v8.0cm/m1/Models\n",
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v8.0cm/m1/Models\\model-350000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v8.0cm/m1/Models\\model-350000.cptk\n",
      "C:\\Users\\Batu\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Batu\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Updated best model checkpoint!\n",
      "Step: 352500. Mean Reward: 0.9487370953874343. Std of Reward: 4.170482148704712.\n",
      "Step: 355000. Mean Reward: 0.8626797599519022. Std of Reward: 4.1228445194690195.\n",
      "Updated best model checkpoint!\n",
      "Step: 357500. Mean Reward: 1.0778588325465333. Std of Reward: 4.259773578895024.\n",
      "Step: 360000. Mean Reward: 0.9736731350666801. Std of Reward: 4.168640159369156.\n",
      "Step: 362500. Mean Reward: 0.9170957120048122. Std of Reward: 4.18961609236072.\n",
      "Step: 365000. Mean Reward: 0.9804547800246014. Std of Reward: 4.18683847832272.\n",
      "Step: 367500. Mean Reward: 1.0667051508534366. Std of Reward: 4.233322035066295.\n",
      "Step: 370000. Mean Reward: 0.9115353294958478. Std of Reward: 4.1283642245445975.\n",
      "Updated best model checkpoint!\n",
      "Step: 372500. Mean Reward: 1.2210780870881661. Std of Reward: 4.352280303310389.\n",
      "Step: 375000. Mean Reward: 1.1413598361289934. Std of Reward: 4.269855842807951.\n",
      "Saved Model\n",
      "Step: 377500. Mean Reward: 1.0048454966002183. Std of Reward: 4.208316130242723.\n",
      "Step: 380000. Mean Reward: 0.9835442728337338. Std of Reward: 4.175636461411312.\n",
      "Step: 382500. Mean Reward: 0.8491950031199261. Std of Reward: 4.085736084965611.\n",
      "Step: 385000. Mean Reward: 0.743926714877256. Std of Reward: 3.9897955954919677.\n",
      "Step: 387500. Mean Reward: 0.6772159359930837. Std of Reward: 3.9227119949287577.\n",
      "Step: 390000. Mean Reward: 0.8246691499378523. Std of Reward: 4.021606565199666.\n",
      "Step: 392500. Mean Reward: 0.6623981818268833. Std of Reward: 3.907106327287476.\n",
      "Step: 395000. Mean Reward: 0.9803355415905396. Std of Reward: 4.185376054496744.\n",
      "Step: 397500. Mean Reward: 1.0778147636110431. Std of Reward: 4.260849676861687.\n",
      "Step: 400000. Mean Reward: 0.7798515048175056. Std of Reward: 3.993108872466582.\n",
      "Saved Model\n",
      "Updated best model checkpoint!\n",
      "Step: 402500. Mean Reward: 1.2298796843785034. Std of Reward: 4.363417524457918.\n",
      "Step: 405000. Mean Reward: 0.8853251175686899. Std of Reward: 4.092272933520164.\n",
      "Step: 407500. Mean Reward: 0.8385260885043435. Std of Reward: 4.055148939061683.\n",
      "Step: 410000. Mean Reward: 0.863025181007097. Std of Reward: 4.10635114078057.\n",
      "Step: 412500. Mean Reward: 1.0528851611259549. Std of Reward: 4.235089712450083.\n",
      "Step: 415000. Mean Reward: 1.186609782265466. Std of Reward: 4.3267082349215285.\n",
      "Step: 417500. Mean Reward: 0.8274409903540406. Std of Reward: 4.048697923580692.\n",
      "Step: 420000. Mean Reward: 1.09713592189677. Std of Reward: 4.28382508687156.\n",
      "Step: 422500. Mean Reward: 1.168600999702996. Std of Reward: 4.3075265320691205.\n",
      "Step: 425000. Mean Reward: 0.9733535310608031. Std of Reward: 4.169242371544256.\n",
      "Saved Model\n",
      "Step: 427500. Mean Reward: 0.8355744086173882. Std of Reward: 4.048189189173902.\n",
      "Updated best model checkpoint!\n",
      "Step: 430000. Mean Reward: 1.2878214139167277. Std of Reward: 4.427080220883614.\n",
      "Step: 432500. Mean Reward: 1.0760085493151987. Std of Reward: 4.259039491247333.\n",
      "Step: 435000. Mean Reward: 0.8912664100080768. Std of Reward: 4.102426038889902.\n",
      "Step: 437500. Mean Reward: 0.7994004014297976. Std of Reward: 4.01359017591056.\n",
      "Step: 440000. Mean Reward: 1.184499309686459. Std of Reward: 4.305675615814131.\n",
      "Step: 442500. Mean Reward: 0.9168564873928489. Std of Reward: 4.13869149588907.\n",
      "Step: 445000. Mean Reward: 0.8858287677172441. Std of Reward: 4.104775208219642.\n",
      "Step: 447500. Mean Reward: 0.8448277244614141. Std of Reward: 4.100816626877185.\n",
      "Step: 450000. Mean Reward: 1.05268851420196. Std of Reward: 4.218233214986832.\n",
      "Saved Model\n",
      "Step: 452500. Mean Reward: 0.9809307680300118. Std of Reward: 4.182467381253656.\n",
      "Updated best model checkpoint!\n",
      "Step: 455000. Mean Reward: 1.3648774492452382. Std of Reward: 4.412361160988168.\n",
      "Step: 457500. Mean Reward: 1.347112357470148. Std of Reward: 4.386192035398945.\n",
      "Step: 460000. Mean Reward: 0.9591098131853538. Std of Reward: 4.188869380021035.\n",
      "Step: 462500. Mean Reward: 1.015925944640857. Std of Reward: 4.212052587036041.\n",
      "Step: 465000. Mean Reward: 1.0616399040005122. Std of Reward: 4.234073794752773.\n",
      "Step: 467500. Mean Reward: 0.725688165250077. Std of Reward: 3.946956296668896.\n",
      "Step: 470000. Mean Reward: 0.7705322389139537. Std of Reward: 3.9621313304117343.\n",
      "Step: 472500. Mean Reward: 1.0431658220286362. Std of Reward: 4.171184599327378.\n",
      "Step: 475000. Mean Reward: 1.0593787470684366. Std of Reward: 4.22836971581173.\n",
      "Saved Model\n",
      "Step: 477500. Mean Reward: 0.8912626308244466. Std of Reward: 4.126527149848584.\n",
      "Step: 480000. Mean Reward: 1.0040068403067894. Std of Reward: 4.19636008001987.\n",
      "Step: 482500. Mean Reward: 0.7502748763747066. Std of Reward: 4.004382817686378.\n",
      "Step: 485000. Mean Reward: 0.9951081352786956. Std of Reward: 4.1790474222715375.\n",
      "Step: 487500. Mean Reward: 0.8671410499613809. Std of Reward: 4.023296701771053.\n",
      "Step: 490000. Mean Reward: 0.7405475486484726. Std of Reward: 3.9421630236342966.\n",
      "Step: 492500. Mean Reward: 0.9471050425320506. Std of Reward: 4.113658676105873.\n",
      "Step: 495000. Mean Reward: 0.6920363329357824. Std of Reward: 3.8975388892776675.\n",
      "Step: 497500. Mean Reward: 0.7009263011464333. Std of Reward: 3.9104335016041625.\n",
      "Step: 500000. Mean Reward: 1.2217413108913784. Std of Reward: 4.314325907361541.\n",
      "Saved Model\n",
      "Step: 502500. Mean Reward: 0.7539093270222336. Std of Reward: 3.9880005394602924.\n",
      "Step: 505000. Mean Reward: 0.6483200411806427. Std of Reward: 3.8219171464433943.\n",
      "Step: 507500. Mean Reward: 1.0407876535730942. Std of Reward: 4.219856293785131.\n",
      "Step: 510000. Mean Reward: 0.9736613896358322. Std of Reward: 4.139840305064696.\n",
      "Step: 512500. Mean Reward: 1.0655822174180092. Std of Reward: 4.206537985821245.\n",
      "Step: 515000. Mean Reward: 0.9806445423083633. Std of Reward: 4.139315309546228.\n",
      "Step: 517500. Mean Reward: 1.0170155193552415. Std of Reward: 4.23843537671578.\n",
      "Step: 520000. Mean Reward: 0.936551578085253. Std of Reward: 4.13598554351452.\n",
      "Step: 522500. Mean Reward: 1.1923530557515536. Std of Reward: 4.283328589138517.\n",
      "Step: 525000. Mean Reward: 0.8084772198063392. Std of Reward: 4.060756769071374.\n",
      "Saved Model\n",
      "Step: 527500. Mean Reward: 0.9249576592251709. Std of Reward: 4.124007317623561.\n",
      "Step: 530000. Mean Reward: 1.1259681046578276. Std of Reward: 4.252223071058214.\n",
      "Step: 532500. Mean Reward: 1.1222388926182272. Std of Reward: 4.295170577034401.\n",
      "Step: 535000. Mean Reward: 0.9164944136932054. Std of Reward: 4.135842351950614.\n",
      "Step: 537500. Mean Reward: 0.9785499952057533. Std of Reward: 4.173993117771649.\n",
      "Step: 540000. Mean Reward: 1.02277949554878. Std of Reward: 4.294765117138167.\n",
      "Step: 542500. Mean Reward: 0.8616814481275968. Std of Reward: 4.140282648964952.\n",
      "Step: 545000. Mean Reward: 0.9064060906099507. Std of Reward: 4.121566500499924.\n",
      "Step: 547500. Mean Reward: 1.2563627439160687. Std of Reward: 4.391372113512316.\n",
      "Step: 550000. Mean Reward: 1.2307890332344902. Std of Reward: 4.407500627272736.\n",
      "Saved Model\n",
      "Step: 552500. Mean Reward: 0.6858783188551737. Std of Reward: 4.00628814920517.\n",
      "Step: 555000. Mean Reward: 0.6468763209987736. Std of Reward: 3.9644479115728037.\n",
      "Step: 557500. Mean Reward: 0.775435237441034. Std of Reward: 4.144566289117805.\n",
      "Step: 560000. Mean Reward: 1.0127477969944678. Std of Reward: 4.3136721399443525.\n",
      "Step: 562500. Mean Reward: 1.1145814621218457. Std of Reward: 4.36256878522262.\n",
      "Step: 565000. Mean Reward: 1.161108635897286. Std of Reward: 4.402660311068627.\n",
      "Step: 567500. Mean Reward: 1.1033802562348656. Std of Reward: 4.311445747084785.\n",
      "Step: 570000. Mean Reward: 1.2093900845625623. Std of Reward: 4.458523477373572.\n",
      "Step: 572500. Mean Reward: 0.8150871819936117. Std of Reward: 4.161073857894049.\n",
      "Step: 575000. Mean Reward: 0.915494147421502. Std of Reward: 4.19920574340253.\n",
      "Saved Model\n",
      "Step: 577500. Mean Reward: 0.9932179581020443. Std of Reward: 4.242299441548953.\n",
      "Step: 580000. Mean Reward: 1.0745691729688627. Std of Reward: 4.353468809840697.\n",
      "Step: 582500. Mean Reward: 1.130637163220429. Std of Reward: 4.357386871560349.\n",
      "Step: 585000. Mean Reward: 0.8543050985375175. Std of Reward: 4.182392308569873.\n",
      "Step: 587500. Mean Reward: 0.7246133289039618. Std of Reward: 4.0770548171958465.\n",
      "Step: 590000. Mean Reward: 1.1409765471139501. Std of Reward: 4.4205530788073375.\n",
      "Step: 592500. Mean Reward: 0.821426874781553. Std of Reward: 4.159728665687446.\n",
      "Step: 595000. Mean Reward: 0.6709875352665239. Std of Reward: 3.9923503544362102.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 597500. Mean Reward: 0.954992021718731. Std of Reward: 4.2654060786910035.\n",
      "Step: 600000. Mean Reward: 1.0952545219566774. Std of Reward: 4.3787410851775785.\n",
      "Saved Model\n",
      "Step: 602500. Mean Reward: 0.8100816117016347. Std of Reward: 4.181896072770293.\n",
      "Step: 605000. Mean Reward: 1.0672393339055077. Std of Reward: 4.380172732017683.\n",
      "Step: 607500. Mean Reward: 0.9066652231202156. Std of Reward: 4.206320305166082.\n",
      "Step: 610000. Mean Reward: 0.6923070191422529. Std of Reward: 4.083419646691783.\n",
      "Step: 612500. Mean Reward: 0.7119825375337558. Std of Reward: 4.1281570921864175.\n",
      "Step: 615000. Mean Reward: 0.9672147830074264. Std of Reward: 4.3289116904904015.\n",
      "Step: 617500. Mean Reward: 1.2993181364771795. Std of Reward: 4.570207455819326.\n",
      "Step: 620000. Mean Reward: 0.9295799628524068. Std of Reward: 4.292352299958648.\n",
      "Step: 622500. Mean Reward: 1.1198695457126395. Std of Reward: 4.424604065835456.\n",
      "Step: 625000. Mean Reward: 0.892660628251767. Std of Reward: 4.249042451131072.\n",
      "Saved Model\n",
      "Step: 627500. Mean Reward: 0.6347197774812171. Std of Reward: 4.0539932914818655.\n",
      "Step: 630000. Mean Reward: 0.47258839873809977. Std of Reward: 3.9214194577346486.\n",
      "Step: 632500. Mean Reward: 0.6036244663046114. Std of Reward: 4.0212505946493415.\n",
      "Step: 635000. Mean Reward: 1.041839400675625. Std of Reward: 4.3586744477791735.\n",
      "Step: 637500. Mean Reward: 1.1570397894705613. Std of Reward: 4.395476069684972.\n",
      "Step: 640000. Mean Reward: 0.7111225902507093. Std of Reward: 4.160887707453748.\n",
      "Step: 642500. Mean Reward: 0.9661604863856822. Std of Reward: 4.346396514811499.\n",
      "Step: 645000. Mean Reward: 0.8745857354429747. Std of Reward: 4.257861207521021.\n",
      "Step: 647500. Mean Reward: 0.5722113264864072. Std of Reward: 4.0109793933597135.\n",
      "Step: 650000. Mean Reward: 0.8086868500805603. Std of Reward: 4.215467610248967.\n",
      "Saved Model\n",
      "Step: 652500. Mean Reward: 0.47397760908543546. Std of Reward: 3.894413001455987.\n",
      "Step: 655000. Mean Reward: 0.9051602707803637. Std of Reward: 4.348834738369173.\n",
      "Step: 657500. Mean Reward: 1.0715803577952732. Std of Reward: 4.42546650721373.\n",
      "Step: 660000. Mean Reward: 0.8413580951912171. Std of Reward: 4.302953355992179.\n",
      "Step: 662500. Mean Reward: 1.0440140547930346. Std of Reward: 4.441211560015942.\n",
      "Step: 665000. Mean Reward: 1.008397566463751. Std of Reward: 4.305514687640481.\n",
      "Step: 667500. Mean Reward: 1.0997282590459094. Std of Reward: 4.468400833829127.\n",
      "Step: 670000. Mean Reward: 1.0453497379082783. Std of Reward: 4.586083229220606.\n",
      "Step: 672500. Mean Reward: 0.13438520009534327. Std of Reward: 3.5699341050309457.\n",
      "Step: 675000. Mean Reward: 0.5385891420979658. Std of Reward: 4.00898122219158.\n",
      "Saved Model\n",
      "Step: 677500. Mean Reward: 0.35899659139081647. Std of Reward: 3.959587413381249.\n",
      "Step: 680000. Mean Reward: 0.39123692604273924. Std of Reward: 3.8761559640380296.\n",
      "Step: 682500. Mean Reward: 0.34633615194370715. Std of Reward: 3.8742799462563373.\n",
      "Step: 685000. Mean Reward: 0.2214892877312206. Std of Reward: 3.879598447790392.\n",
      "Step: 687500. Mean Reward: 0.6527062791811896. Std of Reward: 4.113796264097873.\n",
      "Step: 690000. Mean Reward: 0.4339604048141537. Std of Reward: 3.998200314152707.\n",
      "Step: 692500. Mean Reward: 0.22910844467883643. Std of Reward: 3.7951247453534704.\n",
      "Step: 695000. Mean Reward: 0.5543009060944238. Std of Reward: 4.148844122315516.\n",
      "Step: 697500. Mean Reward: 0.4657170840814572. Std of Reward: 4.023680405965697.\n",
      "Step: 700000. Mean Reward: 1.2584604518120166. Std of Reward: 4.6023973623565455.\n",
      "Saved Model\n",
      "Step: 702500. Mean Reward: 0.41763988658648477. Std of Reward: 3.901546247246923.\n",
      "Step: 705000. Mean Reward: 0.6611101532439265. Std of Reward: 4.156961341168422.\n",
      "Step: 707500. Mean Reward: 0.5695502447189754. Std of Reward: 4.141420187404751.\n",
      "Step: 710000. Mean Reward: 0.7483095209803444. Std of Reward: 4.256300806346.\n",
      "Step: 712500. Mean Reward: 0.8467955740013207. Std of Reward: 4.306382390594452.\n",
      "Updated best model checkpoint!\n",
      "Step: 715000. Mean Reward: 1.7417404226076194. Std of Reward: 4.831046099281238.\n",
      "Step: 717500. Mean Reward: 0.8654825837534554. Std of Reward: 4.317002727555286.\n",
      "Step: 720000. Mean Reward: 0.8077427969050255. Std of Reward: 4.368701605898905.\n",
      "Step: 722500. Mean Reward: 0.9497701164992071. Std of Reward: 4.409273471139281.\n",
      "Step: 725000. Mean Reward: 0.6396411762756844. Std of Reward: 4.13465754634648.\n",
      "Saved Model\n",
      "Step: 727500. Mean Reward: 1.281914054191934. Std of Reward: 4.659696817492282.\n",
      "Step: 730000. Mean Reward: 0.9545166804860367. Std of Reward: 4.417134174120792.\n",
      "Step: 732500. Mean Reward: 0.9130672423229024. Std of Reward: 4.381080764006524.\n",
      "Step: 735000. Mean Reward: 1.267933924061512. Std of Reward: 4.579322842428164.\n",
      "Step: 737500. Mean Reward: 1.2150659970498279. Std of Reward: 4.735185555832161.\n",
      "Step: 740000. Mean Reward: 1.6888838489505507. Std of Reward: 4.809592532989107.\n",
      "Step: 742500. Mean Reward: 1.4236991553313312. Std of Reward: 4.805359244710817.\n",
      "Step: 745000. Mean Reward: 1.14036168674902. Std of Reward: 4.9673287885028605.\n",
      "Step: 747500. Mean Reward: 1.5279652408521636. Std of Reward: 4.821776498304761.\n",
      "Step: 750000. Mean Reward: 1.7329746574164409. Std of Reward: 5.116976290024307.\n",
      "Saved Model\n",
      "Step: 752500. Mean Reward: 0.45555036620883693. Std of Reward: 4.7455289256111515.\n",
      "Step: 755000. Mean Reward: 0.9140167585791197. Std of Reward: 4.562679117314912.\n",
      "Step: 757500. Mean Reward: 1.0521160935416487. Std of Reward: 4.845318098501828.\n",
      "Step: 760000. Mean Reward: 0.840026597089128. Std of Reward: 5.091565323387499.\n",
      "Step: 762500. Mean Reward: 1.13123069938606. Std of Reward: 4.683915177594553.\n",
      "Step: 765000. Mean Reward: 0.5375761569262636. Std of Reward: 4.087075644970179.\n",
      "Step: 767500. Mean Reward: 1.4268672469170796. Std of Reward: 4.691151697249838.\n",
      "Step: 770000. Mean Reward: 1.656840522015489. Std of Reward: 5.058229839133624.\n",
      "Step: 772500. Mean Reward: 1.5257970747527418. Std of Reward: 4.852839912899509.\n",
      "Step: 775000. Mean Reward: 1.7352872671549238. Std of Reward: 5.148998633603327.\n",
      "Saved Model\n",
      "Step: 777500. Mean Reward: 0.8956615803889827. Std of Reward: 4.8855570804540704.\n",
      "Step: 780000. Mean Reward: 1.1226261980643575. Std of Reward: 4.6895781681432025.\n",
      "Updated best model checkpoint!\n",
      "Step: 782500. Mean Reward: 2.3632794272301343. Std of Reward: 5.438788813201798.\n",
      "Step: 785000. Mean Reward: 0.34190404837524413. Std of Reward: 4.572823936223301.\n",
      "Step: 787500. Mean Reward: 1.0397535060268657. Std of Reward: 4.593711637287188.\n",
      "Step: 790000. Mean Reward: 0.9726427151056481. Std of Reward: 4.718764801256218.\n",
      "Step: 792500. Mean Reward: 0.5077548666918145. Std of Reward: 4.4851502686104086.\n",
      "Step: 795000. Mean Reward: 0.9921940282313242. Std of Reward: 4.956856016469322.\n",
      "Step: 797500. Mean Reward: 1.6416366262844895. Std of Reward: 4.947572504458658.\n",
      "Step: 800000. Mean Reward: 0.1578103123856667. Std of Reward: 3.7274101133184296.\n",
      "Saved Model\n",
      "Step: 802500. Mean Reward: 0.8027574560431823. Std of Reward: 5.631405902164378.\n",
      "Step: 805000. Mean Reward: 1.473741237680793. Std of Reward: 4.815269066300171.\n",
      "Step: 807500. Mean Reward: -0.2131554400343287. Std of Reward: 4.257156656308931.\n",
      "Step: 810000. Mean Reward: 1.0741470212024364. Std of Reward: 5.581464775373251.\n",
      "Step: 812500. Mean Reward: 2.086614420119845. Std of Reward: 5.069663172858277.\n",
      "Step: 817500. Mean Reward: -0.5941939380296531. Std of Reward: 4.831605154686467.\n",
      "Step: 820000. Mean Reward: 1.5925586661386102. Std of Reward: 4.745976405260981.\n",
      "Step: 825000. Mean Reward: 0.0795977843284687. Std of Reward: 4.620784066460286.\n",
      "Saved Model\n",
      "Step: 827500. Mean Reward: 0.7921736284509936. Std of Reward: 5.647255067066578.\n",
      "Step: 835000. Mean Reward: 1.7533350339804117. Std of Reward: 6.34283676991171.\n",
      "Updated best model checkpoint!\n",
      "Step: 837500. Mean Reward: 2.5441769746822165. Std of Reward: 5.450347189089862.\n",
      "Updated best model checkpoint!\n",
      "Step: 840000. Mean Reward: 3.118893403830644. Std of Reward: 5.5037341216763656.\n",
      "Step: 842500. Mean Reward: -0.027199034985822993. Std of Reward: 5.425345427254764.\n",
      "Step: 845000. Mean Reward: 1.0707200186786094. Std of Reward: 4.8957302365115964.\n",
      "Step: 847500. Mean Reward: -1.7086445085789335. Std of Reward: 1.2144603265129676.\n",
      "Step: 850000. Mean Reward: -3.3699582189137125. Std of Reward: 5.3257015542098305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 852500. Mean Reward: 1.2009882349414724. Std of Reward: 4.842159125557519.\n",
      "Step: 860000. Mean Reward: -0.7126476479916524. Std of Reward: 6.07362671704991.\n",
      "Step: 862500. Mean Reward: -2.0466139141708286. Std of Reward: 1.0877786156498996.\n",
      "Step: 865000. Mean Reward: -1.5018800277324669. Std of Reward: 0.4463317147233784.\n",
      "Step: 867500. Mean Reward: -1.035549069721745. Std of Reward: 5.429814708072409.\n",
      "Step: 870000. Mean Reward: 1.6421028863938574. Std of Reward: 5.254045093129843.\n",
      "Step: 872500. Mean Reward: -2.0090807235748467. Std of Reward: 1.7631239338987694.\n",
      "Step: 875000. Mean Reward: -2.068927813231185. Std of Reward: 4.616400757701751.\n",
      "Saved Model\n",
      "Step: 877500. Mean Reward: 0.21059732001599343. Std of Reward: 3.669534254653536.\n",
      "Step: 882500. Mean Reward: -4.1007778068262155. Std of Reward: 2.311115083395315.\n",
      "Step: 885000. Mean Reward: -0.7116983750072716. Std of Reward: 5.290462863112912.\n",
      "Step: 887500. Mean Reward: -3.148722101085263. Std of Reward: 1.701307400593077.\n",
      "Step: 890000. Mean Reward: -5.132296792468315. Std of Reward: 2.914304222466645.\n",
      "Step: 892500. Mean Reward: 0.2640070432227492. Std of Reward: 5.609275020394075.\n",
      "Step: 895000. Mean Reward: -1.5169369117667024. Std of Reward: 0.5780893748188234.\n",
      "Step: 900000. Mean Reward: -2.344300618387839. Std of Reward: 4.684345999414692.\n",
      "Saved Model\n",
      "Step: 902500. Mean Reward: -3.75191836059472. Std of Reward: 0.0.\n",
      "Step: 905000. Mean Reward: -1.2357741821313473. Std of Reward: 5.258600548652191.\n",
      "Step: 907500. Mean Reward: -3.1285345880836344. Std of Reward: 3.8529145983011714.\n",
      "Step: 910000. Mean Reward: -0.272089218981318. Std of Reward: 3.490942519312947.\n",
      "Step: 912500. Mean Reward: -2.3930210111647288. Std of Reward: 1.2674896059904983.\n",
      "Step: 917500. Mean Reward: -1.6453871000944638. Std of Reward: 5.8742645833841545.\n",
      "Step: 925000. Mean Reward: -3.6710812799784747. Std of Reward: 4.2872596681593915.\n",
      "Saved Model\n",
      "Step: 930000. Mean Reward: -4.0150762961475195. Std of Reward: 2.8941763878475193.\n",
      "Step: 932500. Mean Reward: -2.327988780996359. Std of Reward: 5.013712936646317.\n",
      "Step: 935000. Mean Reward: -0.031787350543400254. Std of Reward: 3.918903860303182.\n",
      "Step: 937500. Mean Reward: -3.918660414674923. Std of Reward: 2.343210751286998.\n",
      "Step: 942500. Mean Reward: -2.622425939232815. Std of Reward: 4.291007178860593.\n",
      "Step: 950000. Mean Reward: -2.998318361421052. Std of Reward: 5.574723999138074.\n",
      "Saved Model\n",
      "Step: 952500. Mean Reward: -2.8204120234378034. Std of Reward: 1.4295108473478033.\n",
      "Step: 955000. Mean Reward: -4.3254297055376485. Std of Reward: 1.7064534962613198.\n",
      "Step: 957500. Mean Reward: -1.398716427975024. Std of Reward: 6.613365854930661.\n",
      "Step: 960000. Mean Reward: 0.9648396898911055. Std of Reward: 4.66142093164112.\n",
      "Step: 965000. Mean Reward: -0.4197252474615927. Std of Reward: 6.882385071069292.\n",
      "Step: 967500. Mean Reward: 0.14149880008317858. Std of Reward: 3.6399790023636136.\n",
      "Step: 970000. Mean Reward: -2.023112255243329. Std of Reward: 1.797141901845263.\n",
      "Step: 972500. Mean Reward: -4.310155148110566. Std of Reward: 3.0766093142520243.\n",
      "Step: 975000. Mean Reward: -1.6984887422019852. Std of Reward: 4.025462844104258.\n",
      "Saved Model\n",
      "Step: 980000. Mean Reward: -6.456341311303913. Std of Reward: 0.0.\n",
      "Step: 982500. Mean Reward: -1.750265515491428. Std of Reward: 5.117214620751669.\n",
      "Step: 985000. Mean Reward: 1.035230731897392. Std of Reward: 4.684058593123674.\n",
      "Step: 987500. Mean Reward: -2.3985404981925216. Std of Reward: 1.9719721776897203.\n",
      "Step: 990000. Mean Reward: -4.014208588756026. Std of Reward: 2.998799855470216.\n",
      "Step: 997500. Mean Reward: -6.925360325940846. Std of Reward: 2.5961262248829673.\n",
      "Step: 1000000. Mean Reward: 0.08858544810883616. Std of Reward: 3.8541613119198272.\n",
      "Saved Model\n",
      "Step: 1007500. Mean Reward: -6.000721677153679. Std of Reward: 2.907133941003176.\n",
      "Step: 1010000. Mean Reward: -3.1701751850895565. Std of Reward: 1.6633954233519175.\n",
      "Step: 1015000. Mean Reward: -4.487954918881714. Std of Reward: 3.1024728225305003.\n",
      "Step: 1020000. Mean Reward: -3.3358205916488224. Std of Reward: 2.51697445857573.\n",
      "Step: 1022500. Mean Reward: -3.9450707786402344. Std of Reward: 3.1014127753251186.\n",
      "Step: 1025000. Mean Reward: -1.5533818971284679. Std of Reward: 0.37802387349645844.\n",
      "Saved Model\n",
      "Step: 1030000. Mean Reward: -4.145006100559281. Std of Reward: 3.382766560942021.\n",
      "Step: 1032500. Mean Reward: -3.4118535245470376. Std of Reward: 4.044562676917233.\n",
      "Step: 1035000. Mean Reward: -2.2766707219956612. Std of Reward: 1.2643311327636388.\n",
      "Step: 1040000. Mean Reward: -4.465023442792376. Std of Reward: 3.2966242654650317.\n",
      "Step: 1042500. Mean Reward: -2.7650522271366946. Std of Reward: 0.8725161620313743.\n",
      "Step: 1045000. Mean Reward: -4.798718258030117. Std of Reward: 2.3640625493882057.\n",
      "Step: 1047500. Mean Reward: -4.6119549945465685. Std of Reward: 2.9930670058213042.\n",
      "Step: 1050000. Mean Reward: -2.3777267102916704. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1055000. Mean Reward: -6.433206804654723. Std of Reward: 2.8420851467300077.\n",
      "Step: 1057500. Mean Reward: -2.103953944919997. Std of Reward: 0.0.\n",
      "Step: 1065000. Mean Reward: -6.62826262042211. Std of Reward: 2.6797751684581947.\n",
      "Step: 1072500. Mean Reward: -7.028483284315236. Std of Reward: 2.3074728104600646.\n",
      "Saved Model\n",
      "Step: 1077500. Mean Reward: -6.411712364215328. Std of Reward: 0.0.\n",
      "Step: 1080000. Mean Reward: -6.491406662886526. Std of Reward: 2.632728067558355.\n",
      "Step: 1082500. Mean Reward: -2.4886514911212174. Std of Reward: 0.2755628735567246.\n",
      "Step: 1090000. Mean Reward: -7.401340184996761. Std of Reward: 1.592555739665294.\n",
      "Step: 1092500. Mean Reward: -4.743544851036701. Std of Reward: 0.0.\n",
      "Step: 1097500. Mean Reward: -7.1130619296864435. Std of Reward: 2.058116967174609.\n",
      "Saved Model\n",
      "Step: 1105000. Mean Reward: -7.519056910946625. Std of Reward: 2.3320432770788075.\n",
      "Step: 1110000. Mean Reward: -5.703441274073849. Std of Reward: 0.0.\n",
      "Step: 1112500. Mean Reward: -7.7345325254368165. Std of Reward: 1.379777296882593.\n",
      "Step: 1122500. Mean Reward: -6.5524331989918885. Std of Reward: 5.234334081736836.\n",
      "Saved Model\n",
      "Step: 1130000. Mean Reward: -7.6103830303834945. Std of Reward: 1.8489340547553996.\n",
      "Step: 1137500. Mean Reward: -8.122701198523467. Std of Reward: 0.20985541321476184.\n",
      "Step: 1145000. Mean Reward: -8.19403180899543. Std of Reward: 0.04597484706751418.\n",
      "Saved Model\n",
      "Step: 1155000. Mean Reward: -8.185152908348105. Std of Reward: 0.028831263974799903.\n",
      "Step: 1162500. Mean Reward: -8.181352481170602. Std of Reward: 0.038875969055495584.\n",
      "Step: 1170000. Mean Reward: -8.179741455544123. Std of Reward: 0.045563866961352825.\n",
      "Step: 1172500. Mean Reward: -2.9321265444921707. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1180000. Mean Reward: -8.015410002687219. Std of Reward: 0.5632794463943539.\n",
      "Step: 1187500. Mean Reward: -8.199413691059199. Std of Reward: 0.03875693833399623.\n",
      "Step: 1195000. Mean Reward: -8.215465248858147. Std of Reward: 0.029001225084642552.\n",
      "Saved Model\n",
      "Step: 1202500. Mean Reward: -8.193433004833755. Std of Reward: 0.02500081859450993.\n",
      "Step: 1212500. Mean Reward: -8.19812252601748. Std of Reward: 0.024781963193556085.\n",
      "Step: 1220000. Mean Reward: -8.21045181763321. Std of Reward: 0.028673768839779516.\n",
      "Saved Model\n",
      "Step: 1227500. Mean Reward: -8.194032445699937. Std of Reward: 0.02576208220480025.\n",
      "Step: 1237500. Mean Reward: -8.190383113075326. Std of Reward: 0.023444855056564157.\n",
      "Step: 1245000. Mean Reward: -8.205368160052204. Std of Reward: 0.024742502333312388.\n",
      "Saved Model\n",
      "Step: 1252500. Mean Reward: -8.188956606760389. Std of Reward: 0.03081551745851275.\n",
      "Step: 1260000. Mean Reward: -8.204321368664218. Std of Reward: 0.035006206507986075.\n",
      "Step: 1267500. Mean Reward: -7.403600185236565. Std of Reward: 0.0.\n",
      "Step: 1270000. Mean Reward: -7.563631167811702. Std of Reward: 1.9292461879935283.\n",
      "Saved Model\n",
      "Step: 1277500. Mean Reward: -8.187230685676749. Std of Reward: 0.02865551624056161.\n",
      "Step: 1285000. Mean Reward: -8.205708812026264. Std of Reward: 0.023461612212125574.\n",
      "Step: 1292500. Mean Reward: -7.546465908980057. Std of Reward: 2.0700397915507276.\n",
      "Saved Model\n",
      "Step: 1302500. Mean Reward: -8.199458511426112. Std of Reward: 0.024260096871657814.\n",
      "Step: 1310000. Mean Reward: -8.191237452884518. Std of Reward: 0.021823511209681037.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1317500. Mean Reward: -8.187288058373777. Std of Reward: 0.020317524945039744.\n",
      "Saved Model\n",
      "Step: 1327500. Mean Reward: -8.199890597333622. Std of Reward: 0.02830212924676973.\n",
      "Step: 1335000. Mean Reward: -8.193341296667056. Std of Reward: 0.024746072768442335.\n",
      "Step: 1342500. Mean Reward: -8.19683414736585. Std of Reward: 0.021451135806979146.\n",
      "Step: 1350000. Mean Reward: -8.18941566697821. Std of Reward: 0.025978829453764604.\n",
      "Saved Model\n",
      "Step: 1360000. Mean Reward: -8.198374826254677. Std of Reward: 0.023105353276510535.\n",
      "Step: 1367500. Mean Reward: -8.188401681602661. Std of Reward: 0.022028740399871946.\n",
      "Step: 1375000. Mean Reward: -8.1818220917804. Std of Reward: 0.019253404066350528.\n",
      "Saved Model\n",
      "Step: 1385000. Mean Reward: -8.195146791091508. Std of Reward: 0.025685281112680917.\n",
      "Step: 1392500. Mean Reward: -8.192873193350415. Std of Reward: 0.027459169383644247.\n",
      "Step: 1400000. Mean Reward: -8.191256677168422. Std of Reward: 0.01774603374222038.\n",
      "Saved Model\n",
      "Step: 1407500. Mean Reward: -8.194910737444658. Std of Reward: 0.024644645314058692.\n",
      "Step: 1417500. Mean Reward: -8.18867504267448. Std of Reward: 0.022419134748750468.\n",
      "Step: 1425000. Mean Reward: -8.20759925985074. Std of Reward: 0.017985728120688965.\n",
      "Saved Model\n",
      "Step: 1432500. Mean Reward: -8.19858425290526. Std of Reward: 0.01821518527225683.\n",
      "Step: 1440000. Mean Reward: -8.187510796971733. Std of Reward: 0.019870490482240856.\n",
      "Step: 1450000. Mean Reward: -8.197589074602433. Std of Reward: 0.02171579164737842.\n",
      "Saved Model\n",
      "Step: 1457500. Mean Reward: -8.190717546042382. Std of Reward: 0.022956629811858593.\n",
      "Step: 1465000. Mean Reward: -8.202808381489593. Std of Reward: 0.019767308812729593.\n",
      "Step: 1475000. Mean Reward: -8.186386071846513. Std of Reward: 0.02207942059958628.\n",
      "Saved Model\n",
      "Step: 1482500. Mean Reward: -8.190048906446467. Std of Reward: 0.024122871396877542.\n",
      "Step: 1490000. Mean Reward: -8.179998552658422. Std of Reward: 0.024431387067365966.\n",
      "Step: 1497500. Mean Reward: -8.198980524167204. Std of Reward: 0.028297860308200064.\n",
      "Saved Model\n",
      "Step: 1507500. Mean Reward: -8.18391899313589. Std of Reward: 0.023289640012952837.\n",
      "Step: 1515000. Mean Reward: -8.189060060673253. Std of Reward: 0.017095284121994875.\n",
      "Step: 1522500. Mean Reward: -7.5487533843957575. Std of Reward: 2.0719017128747734.\n",
      "Saved Model\n",
      "Step: 1532500. Mean Reward: -8.198945569333906. Std of Reward: 0.024622457129505747.\n",
      "Step: 1540000. Mean Reward: -8.20651272000419. Std of Reward: 0.017124435898783087.\n",
      "Step: 1547500. Mean Reward: -8.199332887422596. Std of Reward: 0.02728059287081114.\n",
      "Saved Model\n",
      "Step: 1555000. Mean Reward: -8.1877496354894. Std of Reward: 0.021776940764478576.\n",
      "Step: 1565000. Mean Reward: -8.20035957486498. Std of Reward: 0.022193226117398792.\n",
      "Step: 1572500. Mean Reward: -8.19351619879939. Std of Reward: 0.023002081212152303.\n",
      "Saved Model\n",
      "Step: 1580000. Mean Reward: -8.200392419676236. Std of Reward: 0.02375257317558901.\n",
      "Step: 1587500. Mean Reward: -8.207316056944206. Std of Reward: 0.020033455259248826.\n",
      "Step: 1597500. Mean Reward: -8.189900514366004. Std of Reward: 0.02200198000414048.\n",
      "Saved Model\n",
      "Step: 1605000. Mean Reward: -8.194488565343326. Std of Reward: 0.01726930766526798.\n",
      "Step: 1612500. Mean Reward: -8.197225420115917. Std of Reward: 0.02522901482443647.\n",
      "Step: 1622500. Mean Reward: -8.199953118915113. Std of Reward: 0.023405797639900004.\n",
      "Saved Model\n",
      "Step: 1630000. Mean Reward: -8.19199630899671. Std of Reward: 0.026343673757301678.\n",
      "Step: 1637500. Mean Reward: -8.186676484745234. Std of Reward: 0.028554996947052047.\n",
      "Step: 1645000. Mean Reward: -8.186398812851255. Std of Reward: 0.02288228058935162.\n",
      "Saved Model\n",
      "Step: 1655000. Mean Reward: -7.544731402549346. Std of Reward: 2.0620253330129423.\n",
      "Step: 1662500. Mean Reward: -8.195302206804799. Std of Reward: 0.021561948539128836.\n",
      "Step: 1670000. Mean Reward: -8.199138834555956. Std of Reward: 0.018517440675986405.\n",
      "Saved Model\n",
      "Step: 1680000. Mean Reward: -8.194667322188536. Std of Reward: 0.02459225919385669.\n",
      "Step: 1687500. Mean Reward: -8.195520402718875. Std of Reward: 0.028309331359338937.\n",
      "Step: 1695000. Mean Reward: -8.198034211889034. Std of Reward: 0.015460208978068404.\n",
      "Saved Model\n",
      "Step: 1702500. Mean Reward: -8.195769616988898. Std of Reward: 0.024804489967394743.\n",
      "Step: 1712500. Mean Reward: -8.204020806831887. Std of Reward: 0.022682348367048705.\n",
      "Step: 1720000. Mean Reward: -8.216659395015215. Std of Reward: 0.017494848095089183.\n",
      "Saved Model\n",
      "Step: 1727500. Mean Reward: -7.535465222545117. Std of Reward: 2.066490795620309.\n",
      "Step: 1735000. Mean Reward: -8.187610899576713. Std of Reward: 0.015052306374020104.\n",
      "Step: 1745000. Mean Reward: -8.184624406361035. Std of Reward: 0.026902977644273025.\n",
      "Saved Model\n",
      "Step: 1752500. Mean Reward: -8.19648647478665. Std of Reward: 0.020751979006908495.\n",
      "Step: 1760000. Mean Reward: -8.194374581953712. Std of Reward: 0.023275311461271455.\n",
      "Step: 1770000. Mean Reward: -8.198724442812876. Std of Reward: 0.026961842967586556.\n",
      "Saved Model\n",
      "Step: 1777500. Mean Reward: -8.189550821127336. Std of Reward: 0.021150353838137965.\n",
      "Step: 1785000. Mean Reward: -8.200146180699912. Std of Reward: 0.02391762358188837.\n",
      "Step: 1792500. Mean Reward: -8.209806863128321. Std of Reward: 0.02240971623060409.\n",
      "Saved Model\n",
      "Step: 1802500. Mean Reward: -7.540654267702936. Std of Reward: 2.0647015989767263.\n",
      "Step: 1810000. Mean Reward: -8.194848776033732. Std of Reward: 0.028381883721016528.\n",
      "Step: 1817500. Mean Reward: -8.18999584333177. Std of Reward: 0.030261975519029052.\n",
      "Saved Model\n",
      "Step: 1827500. Mean Reward: -8.194394265707675. Std of Reward: 0.020406227279576427.\n",
      "Step: 1835000. Mean Reward: -8.193188784563286. Std of Reward: 0.020212762853432095.\n",
      "Step: 1842500. Mean Reward: -8.199975476090538. Std of Reward: 0.028187560519752977.\n",
      "Step: 1850000. Mean Reward: -8.214306606093862. Std of Reward: 0.031001211108673647.\n",
      "Saved Model\n",
      "Step: 1860000. Mean Reward: -8.19816227331573. Std of Reward: 0.02961623598911922.\n",
      "Step: 1867500. Mean Reward: -8.189689296438099. Std of Reward: 0.030760594972522884.\n",
      "Step: 1875000. Mean Reward: -8.19354412476516. Std of Reward: 0.02447492273240793.\n",
      "Saved Model\n",
      "Step: 1882500. Mean Reward: -8.195797465694334. Std of Reward: 0.02837909211602681.\n",
      "Step: 1892500. Mean Reward: -8.197797435604757. Std of Reward: 0.025832783947581005.\n",
      "Step: 1900000. Mean Reward: -6.556330809209697. Std of Reward: 5.229445028821795.\n",
      "Saved Model\n",
      "Step: 1907500. Mean Reward: -8.206057460507399. Std of Reward: 0.02863407591985969.\n",
      "Step: 1917500. Mean Reward: -8.20769264701694. Std of Reward: 0.026048020384851978.\n",
      "Step: 1925000. Mean Reward: -8.200539822460792. Std of Reward: 0.024612775419317254.\n",
      "Saved Model\n",
      "Step: 1932500. Mean Reward: -8.193731961634754. Std of Reward: 0.020627674138716988.\n",
      "Step: 1940000. Mean Reward: -8.19165058719955. Std of Reward: 0.02927945923738632.\n",
      "Step: 1950000. Mean Reward: -8.201414266285092. Std of Reward: 0.015864612853274427.\n",
      "Saved Model\n",
      "Step: 1955000. Mean Reward: -6.475657845501765. Std of Reward: 0.0.\n",
      "Step: 1957500. Mean Reward: -7.633938797478076. Std of Reward: 1.6548189153969495.\n",
      "Step: 1965000. Mean Reward: -8.178381008047086. Std of Reward: 0.025294782757686204.\n",
      "Step: 1975000. Mean Reward: -8.194928057718922. Std of Reward: 0.02149929649788767.\n",
      "Saved Model\n",
      "Step: 1982500. Mean Reward: -8.186242373804818. Std of Reward: 0.016118956872203768.\n",
      "Step: 1990000. Mean Reward: -8.19291824700244. Std of Reward: 0.029899825158227075.\n",
      "Step: 1997500. Mean Reward: -8.179540867936643. Std of Reward: 0.031650828395482576.\n",
      "Saved Model\n",
      "Step: 2007500. Mean Reward: -8.168127180808373. Std of Reward: 0.02619662546350537.\n",
      "Step: 2015000. Mean Reward: -8.189079368305906. Std of Reward: 0.022743650105105838.\n",
      "Step: 2022500. Mean Reward: -8.205563556381671. Std of Reward: 0.024797535986781912.\n",
      "Saved Model\n",
      "Step: 2030000. Mean Reward: -8.211647385522667. Std of Reward: 0.030174248968670284.\n",
      "Step: 2040000. Mean Reward: -8.207095382537144. Std of Reward: 0.020949460242633822.\n",
      "Step: 2047500. Mean Reward: -6.623125882163849. Std of Reward: 4.956837105580559.\n",
      "Saved Model\n",
      "Step: 2055000. Mean Reward: -8.113058477210608. Std of Reward: 0.29112794330505587.\n",
      "Step: 2065000. Mean Reward: -8.208805396024074. Std of Reward: 0.03433551997399914.\n",
      "Step: 2072500. Mean Reward: -8.198600088243719. Std of Reward: 0.024445308265466818.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 2080000. Mean Reward: -8.199489141059932. Std of Reward: 0.03416807000039318.\n",
      "Step: 2087500. Mean Reward: -8.187995495507808. Std of Reward: 0.020370871694462354.\n",
      "Step: 2097500. Mean Reward: -8.178441549388825. Std of Reward: 0.03107109875558689.\n",
      "Saved Model\n",
      "Step: 2105000. Mean Reward: -6.536791041438215. Std of Reward: 5.229098770477962.\n",
      "Step: 2112500. Mean Reward: -8.205877122617704. Std of Reward: 0.013996429742515007.\n",
      "Step: 2122500. Mean Reward: -8.204116939914597. Std of Reward: 0.0234025318247967.\n",
      "Saved Model\n",
      "Step: 2130000. Mean Reward: -8.191088756926092. Std of Reward: 0.02909264445131662.\n",
      "Step: 2137500. Mean Reward: -8.175593724569017. Std of Reward: 0.02012793897418219.\n",
      "Step: 2145000. Mean Reward: -8.194619757175289. Std of Reward: 0.033031000674988306.\n",
      "Saved Model\n",
      "Step: 2155000. Mean Reward: -8.19058163189402. Std of Reward: 0.03369382180296558.\n",
      "Step: 2162500. Mean Reward: -8.199571843224149. Std of Reward: 0.030135964687826217.\n",
      "Step: 2170000. Mean Reward: -7.531832765087464. Std of Reward: 2.065359578372405.\n",
      "Saved Model\n",
      "Step: 2177500. Mean Reward: -8.193491142655926. Std of Reward: 0.0199994609383744.\n",
      "Step: 2187500. Mean Reward: -8.207672847203254. Std of Reward: 0.019127485404791388.\n",
      "Step: 2195000. Mean Reward: -8.18070072431481. Std of Reward: 0.014553323215569923.\n",
      "Saved Model\n",
      "Step: 2202500. Mean Reward: -8.194213602404469. Std of Reward: 0.025201077197291366.\n",
      "Step: 2212500. Mean Reward: -8.18334472939733. Std of Reward: 0.02328441170307944.\n",
      "Step: 2220000. Mean Reward: -8.184423564430244. Std of Reward: 0.017948584853327666.\n",
      "Saved Model\n",
      "Step: 2227500. Mean Reward: -8.188277438078808. Std of Reward: 0.022882352506102314.\n",
      "Step: 2235000. Mean Reward: -8.187916147961946. Std of Reward: 0.026560562133390003.\n",
      "Step: 2245000. Mean Reward: -8.20236435173324. Std of Reward: 0.022790414549994087.\n",
      "Saved Model\n",
      "Step: 2252500. Mean Reward: -8.194565279219656. Std of Reward: 0.03151913331909579.\n",
      "Step: 2260000. Mean Reward: -8.186587564588772. Std of Reward: 0.0233898952753371.\n",
      "Step: 2270000. Mean Reward: -8.184685403764595. Std of Reward: 0.018052239518397097.\n",
      "Saved Model\n",
      "Step: 2277500. Mean Reward: -8.201621820371026. Std of Reward: 0.02383435931233368.\n",
      "Step: 2285000. Mean Reward: -8.197659380243532. Std of Reward: 0.024992663207503582.\n",
      "Step: 2292500. Mean Reward: -8.19272434444677. Std of Reward: 0.02346188745871565.\n",
      "Saved Model\n",
      "Step: 2302500. Mean Reward: -8.202973215363391. Std of Reward: 0.022434765536326083.\n",
      "Step: 2310000. Mean Reward: -8.195666543986746. Std of Reward: 0.02461859492461294.\n",
      "Step: 2317500. Mean Reward: -8.182591668813963. Std of Reward: 0.020190464831786305.\n",
      "Step: 2325000. Mean Reward: -8.188804323222492. Std of Reward: 0.022864124723255864.\n",
      "Saved Model\n",
      "Step: 2335000. Mean Reward: -8.18684955767393. Std of Reward: 0.025294066162242754.\n",
      "Step: 2342500. Mean Reward: -8.206614953576864. Std of Reward: 0.02093249875038549.\n",
      "Step: 2350000. Mean Reward: -8.192413827405009. Std of Reward: 0.022015033131786294.\n",
      "Saved Model\n",
      "Step: 2360000. Mean Reward: -8.189597741867281. Std of Reward: 0.02412661917501511.\n",
      "Step: 2367500. Mean Reward: -8.185215379674704. Std of Reward: 0.018170437649053708.\n",
      "Step: 2375000. Mean Reward: -8.190958215935844. Std of Reward: 0.03475654378028541.\n",
      "Saved Model\n",
      "Step: 2382500. Mean Reward: -8.20272354351481. Std of Reward: 0.024810829348338642.\n",
      "Step: 2392500. Mean Reward: -8.208693773392913. Std of Reward: 0.029554132558347185.\n",
      "Step: 2400000. Mean Reward: -8.193674966983298. Std of Reward: 0.025043497597283533.\n",
      "Saved Model\n",
      "Step: 2407500. Mean Reward: -8.179868242527549. Std of Reward: 0.022208042125606044.\n",
      "Step: 2417500. Mean Reward: -8.199290315416125. Std of Reward: 0.024846332799903496.\n",
      "Step: 2425000. Mean Reward: -8.19309517059739. Std of Reward: 0.02630443220047009.\n",
      "Saved Model\n",
      "Step: 2432500. Mean Reward: -8.206827355850535. Std of Reward: 0.02539626840855783.\n",
      "Step: 2440000. Mean Reward: -7.536018631247327. Std of Reward: 2.066717489180617.\n",
      "Step: 2450000. Mean Reward: -8.19434471756863. Std of Reward: 0.019181829842779084.\n",
      "Saved Model\n",
      "Step: 2457500. Mean Reward: -8.197028352296716. Std of Reward: 0.02032057754894376.\n",
      "Step: 2465000. Mean Reward: -8.178886262779509. Std of Reward: 0.022180138864682394.\n",
      "Step: 2475000. Mean Reward: -6.534655223797156. Std of Reward: 5.22845389086058.\n",
      "Saved Model\n",
      "Step: 2482500. Mean Reward: -8.200239722042292. Std of Reward: 0.019421849256787162.\n",
      "Step: 2490000. Mean Reward: -8.191600152997822. Std of Reward: 0.02489401342066571.\n",
      "Step: 2497500. Mean Reward: -8.199896547843766. Std of Reward: 0.02532446377954014.\n",
      "Saved Model\n",
      "Step: 2507500. Mean Reward: -8.203973276287932. Std of Reward: 0.02272933293991613.\n",
      "Step: 2515000. Mean Reward: -8.189367678038959. Std of Reward: 0.013084020085765834.\n",
      "Step: 2522500. Mean Reward: -8.198470231954792. Std of Reward: 0.02545367886553477.\n",
      "Saved Model\n",
      "Step: 2530000. Mean Reward: -8.190840456392912. Std of Reward: 0.02724334825541842.\n",
      "Step: 2540000. Mean Reward: -8.206210138625554. Std of Reward: 0.023320672238725606.\n",
      "Step: 2547500. Mean Reward: -8.189395296198047. Std of Reward: 0.0265907447545949.\n",
      "Saved Model\n",
      "Step: 2555000. Mean Reward: -8.189414254848808. Std of Reward: 0.02147600252754719.\n",
      "Step: 2565000. Mean Reward: -8.198058664253761. Std of Reward: 0.02903952675389464.\n",
      "Step: 2572500. Mean Reward: -8.19951576405187. Std of Reward: 0.028665892134069823.\n",
      "Saved Model\n",
      "Step: 2580000. Mean Reward: -8.199448205969913. Std of Reward: 0.02901024468938513.\n",
      "Step: 2587500. Mean Reward: -8.194454767205078. Std of Reward: 0.022038914742200277.\n",
      "Step: 2597500. Mean Reward: -8.203177034410336. Std of Reward: 0.02788757483696912.\n",
      "Saved Model\n",
      "Step: 2605000. Mean Reward: -8.207474596451297. Std of Reward: 0.019184453316383035.\n",
      "Step: 2612500. Mean Reward: -8.198066064996445. Std of Reward: 0.025110026494398155.\n",
      "Step: 2622500. Mean Reward: -8.21381897136305. Std of Reward: 0.018741221789768468.\n",
      "Saved Model\n",
      "Step: 2630000. Mean Reward: -8.19505516211532. Std of Reward: 0.028150749787577586.\n",
      "Step: 2637500. Mean Reward: -8.209365655847671. Std of Reward: 0.02065362512012584.\n",
      "Step: 2645000. Mean Reward: -8.209417726703938. Std of Reward: 0.02752096941466503.\n",
      "Saved Model\n",
      "Step: 2655000. Mean Reward: -8.204873010597526. Std of Reward: 0.025737094367572596.\n",
      "Step: 2662500. Mean Reward: -8.198977548513453. Std of Reward: 0.023996578816738064.\n",
      "Step: 2670000. Mean Reward: -8.203484346370306. Std of Reward: 0.02097032435608754.\n",
      "Saved Model\n",
      "Step: 2677500. Mean Reward: -8.186119996896128. Std of Reward: 0.019290594292180295.\n",
      "Step: 2687500. Mean Reward: -8.20055370546636. Std of Reward: 0.015881844263452113.\n",
      "Step: 2695000. Mean Reward: -8.207333413884356. Std of Reward: 0.019990762606763378.\n",
      "Saved Model\n",
      "Step: 2702500. Mean Reward: -8.195735589746885. Std of Reward: 0.02414139703780398.\n",
      "Step: 2712500. Mean Reward: -8.186816942027063. Std of Reward: 0.024122570176363337.\n",
      "Step: 2720000. Mean Reward: -8.197269822954933. Std of Reward: 0.02552647844257526.\n",
      "Saved Model\n",
      "Step: 2727500. Mean Reward: -8.191942331140982. Std of Reward: 0.02564289287793235.\n",
      "Step: 2735000. Mean Reward: -8.1985852830699. Std of Reward: 0.02410642031699871.\n",
      "Step: 2745000. Mean Reward: -8.184847604388304. Std of Reward: 0.023243888697281848.\n",
      "Saved Model\n",
      "Step: 2752500. Mean Reward: -8.196532861327308. Std of Reward: 0.01469641426884067.\n",
      "Step: 2760000. Mean Reward: -8.186868707257755. Std of Reward: 0.02640512660208238.\n",
      "Step: 2770000. Mean Reward: -8.198723794582005. Std of Reward: 0.0186319646994848.\n",
      "Saved Model\n",
      "Step: 2777500. Mean Reward: -8.2082434479385. Std of Reward: 0.015405142327336183.\n",
      "Step: 2785000. Mean Reward: -8.187361261081607. Std of Reward: 0.02158000513691934.\n",
      "Step: 2792500. Mean Reward: -8.18417026676446. Std of Reward: 0.020362216149230897.\n",
      "Saved Model\n",
      "Step: 2802500. Mean Reward: -8.18926056062878. Std of Reward: 0.025021795498208127.\n",
      "Step: 2810000. Mean Reward: -8.202226258752633. Std of Reward: 0.023688296107078073.\n",
      "Step: 2817500. Mean Reward: -8.189589234834594. Std of Reward: 0.016247941356258788.\n",
      "Step: 2825000. Mean Reward: -8.200367356981674. Std of Reward: 0.026327803454134952.\n",
      "Saved Model\n",
      "Step: 2835000. Mean Reward: -8.20891658922309. Std of Reward: 0.014348275655368939.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2842500. Mean Reward: -8.184598466742642. Std of Reward: 0.021034760227872616.\n",
      "Step: 2850000. Mean Reward: -8.204175035021652. Std of Reward: 0.023520385109492427.\n",
      "Saved Model\n",
      "Step: 2860000. Mean Reward: -8.19146885088247. Std of Reward: 0.02270566628614662.\n",
      "Step: 2867500. Mean Reward: -7.53461652178247. Std of Reward: 2.0662390107477564.\n",
      "Step: 2875000. Mean Reward: -8.183125540274027. Std of Reward: 0.01986075070314788.\n",
      "Saved Model\n",
      "Step: 2882500. Mean Reward: -6.546913957037631. Std of Reward: 5.231650112200076.\n",
      "Step: 2892500. Mean Reward: -8.192281161424013. Std of Reward: 0.026223809472764474.\n",
      "Step: 2900000. Mean Reward: -8.191492845021992. Std of Reward: 0.025611111637478708.\n",
      "Saved Model\n",
      "Step: 2907500. Mean Reward: -8.184398652938935. Std of Reward: 0.018811630524704998.\n",
      "Step: 2917500. Mean Reward: -8.187071649266572. Std of Reward: 0.020205501293990753.\n",
      "Step: 2925000. Mean Reward: -8.190474511506196. Std of Reward: 0.02552282564496973.\n",
      "Saved Model\n",
      "Step: 2932500. Mean Reward: -8.204135858029812. Std of Reward: 0.027587660529557953.\n",
      "Step: 2940000. Mean Reward: -8.187367288154665. Std of Reward: 0.02050585462923153.\n",
      "Step: 2950000. Mean Reward: -8.194101335209908. Std of Reward: 0.021504922128165987.\n",
      "Saved Model\n",
      "Step: 2957500. Mean Reward: -8.191470605213478. Std of Reward: 0.02568019429433979.\n",
      "Step: 2965000. Mean Reward: -8.213280831946188. Std of Reward: 0.016166764785089265.\n",
      "Step: 2972500. Mean Reward: -8.195769235016012. Std of Reward: 0.02001703257929822.\n",
      "Saved Model\n",
      "Step: 2982500. Mean Reward: -8.200812951620234. Std of Reward: 0.022098104705334843.\n",
      "Step: 2990000. Mean Reward: -8.194916650597628. Std of Reward: 0.025723155211226974.\n",
      "Step: 2997500. Mean Reward: -8.191265909178991. Std of Reward: 0.022830688190784394.\n",
      "Saved Model\n",
      "Step: 3007500. Mean Reward: -8.183762285132058. Std of Reward: 0.021938878645313745.\n",
      "Step: 3015000. Mean Reward: -8.201118738127644. Std of Reward: 0.02286911539050017.\n",
      "Step: 3022500. Mean Reward: -8.187596551813254. Std of Reward: 0.01827797017400785.\n",
      "Saved Model\n",
      "Step: 3030000. Mean Reward: -8.185971261547902. Std of Reward: 0.017058168220581465.\n",
      "Step: 3040000. Mean Reward: -8.197775575067897. Std of Reward: 0.023525964756555485.\n",
      "Step: 3047500. Mean Reward: -8.203897630265503. Std of Reward: 0.026180604613057254.\n",
      "Saved Model\n",
      "Step: 3055000. Mean Reward: -8.200945397760844. Std of Reward: 0.021066281935937386.\n",
      "Step: 3065000. Mean Reward: -8.195075494325188. Std of Reward: 0.017939632478358808.\n",
      "Step: 3072500. Mean Reward: -8.182582742459855. Std of Reward: 0.02290993683561404.\n",
      "Saved Model\n",
      "Step: 3080000. Mean Reward: -8.212110864669413. Std of Reward: 0.021787637373127316.\n",
      "Step: 3087500. Mean Reward: -8.196463815681577. Std of Reward: 0.020635271214838113.\n",
      "Step: 3097500. Mean Reward: -8.197144090438707. Std of Reward: 0.027902335948649916.\n",
      "Saved Model\n",
      "Step: 3105000. Mean Reward: -8.202953264017902. Std of Reward: 0.025160950331101353.\n",
      "Step: 3112500. Mean Reward: -8.197873575083285. Std of Reward: 0.02448571172908774.\n",
      "Step: 3120000. Mean Reward: -8.19462135494449. Std of Reward: 0.024317051704827407.\n",
      "Saved Model\n",
      "Step: 3130000. Mean Reward: -8.196089364888774. Std of Reward: 0.02595778899423772.\n",
      "Step: 3137500. Mean Reward: -8.198830224574026. Std of Reward: 0.027641595743246327.\n",
      "Step: 3145000. Mean Reward: -8.193091012837778. Std of Reward: 0.02447051758627083.\n",
      "Saved Model\n",
      "Step: 3155000. Mean Reward: -8.190353738634611. Std of Reward: 0.02307683257270259.\n",
      "Step: 3162500. Mean Reward: -7.5445064711388135. Std of Reward: 2.0693687465801838.\n",
      "Step: 3170000. Mean Reward: -8.199334341577174. Std of Reward: 0.0240953572500268.\n",
      "Saved Model\n",
      "Step: 3177500. Mean Reward: -8.19202728443584. Std of Reward: 0.015869817732564972.\n",
      "Step: 3187500. Mean Reward: -8.199174882900124. Std of Reward: 0.026724990321470722.\n",
      "Step: 3195000. Mean Reward: -8.193187982493539. Std of Reward: 0.023829545201815832.\n",
      "Saved Model\n",
      "Step: 3202500. Mean Reward: -8.177183153493587. Std of Reward: 0.023102400774455325.\n",
      "Step: 3212500. Mean Reward: -8.183159296062854. Std of Reward: 0.0230044518884578.\n",
      "Step: 3220000. Mean Reward: -8.187407380829331. Std of Reward: 0.01657328534325378.\n",
      "Saved Model\n",
      "Step: 3227500. Mean Reward: -8.19748275911777. Std of Reward: 0.018715136641175446.\n",
      "Step: 3235000. Mean Reward: -8.193544389444614. Std of Reward: 0.01944083615756751.\n",
      "Step: 3245000. Mean Reward: -8.202646142504856. Std of Reward: 0.01949361743129484.\n",
      "Saved Model\n",
      "Step: 3252500. Mean Reward: -8.215746652220298. Std of Reward: 0.012115702979955628.\n",
      "Step: 3260000. Mean Reward: -8.215073854069377. Std of Reward: 0.019908453006001237.\n",
      "Step: 3267500. Mean Reward: -8.200802613708227. Std of Reward: 0.018048750394567454.\n",
      "Saved Model\n",
      "Step: 3277500. Mean Reward: -8.19150219104416. Std of Reward: 0.02727696948145244.\n",
      "Step: 3285000. Mean Reward: -8.18422947076167. Std of Reward: 0.020183267440858688.\n",
      "Step: 3292500. Mean Reward: -8.193339387459812. Std of Reward: 0.02341589491006365.\n",
      "Saved Model\n",
      "Step: 3302500. Mean Reward: -8.201243821440444. Std of Reward: 0.024761811853078783.\n",
      "Step: 3310000. Mean Reward: -8.19042488240698. Std of Reward: 0.018469802201379844.\n",
      "Step: 3317500. Mean Reward: -8.195287324592178. Std of Reward: 0.023331182745590394.\n",
      "Step: 3325000. Mean Reward: -8.189803468201514. Std of Reward: 0.02332520869229947.\n",
      "Saved Model\n",
      "Step: 3335000. Mean Reward: -8.206196175913114. Std of Reward: 0.02014576497703241.\n",
      "Step: 3342500. Mean Reward: -8.194979020495106. Std of Reward: 0.02718010066477457.\n",
      "Step: 3350000. Mean Reward: -8.200294806103326. Std of Reward: 0.028299919416591063.\n",
      "Saved Model\n",
      "Step: 3360000. Mean Reward: -8.194243398065476. Std of Reward: 0.026703006417716668.\n",
      "Step: 3367500. Mean Reward: -8.18624500889459. Std of Reward: 0.03414857002228313.\n",
      "Step: 3375000. Mean Reward: -8.194596791340732. Std of Reward: 0.023175467308606493.\n",
      "Saved Model\n",
      "Step: 3382500. Mean Reward: -8.179003261547926. Std of Reward: 0.029681206128283528.\n",
      "Step: 3392500. Mean Reward: -8.181407016347428. Std of Reward: 0.015008114044737989.\n",
      "Step: 3400000. Mean Reward: -8.196592945239242. Std of Reward: 0.01967137927113543.\n",
      "Saved Model\n",
      "Step: 3407500. Mean Reward: -8.196039966219914. Std of Reward: 0.02532134937615767.\n",
      "Step: 3415000. Mean Reward: -8.194934924741649. Std of Reward: 0.021997704771686867.\n",
      "Step: 3425000. Mean Reward: -8.19451595617804. Std of Reward: 0.01890183158326532.\n",
      "Saved Model\n",
      "Step: 3432500. Mean Reward: -8.203516162129649. Std of Reward: 0.023272765878648925.\n",
      "Step: 3440000. Mean Reward: -8.212404063241701. Std of Reward: 0.02766297707810801.\n",
      "Step: 3450000. Mean Reward: -8.195428278098769. Std of Reward: 0.026177105130756904.\n",
      "Saved Model\n",
      "Step: 3457500. Mean Reward: -8.201132509032053. Std of Reward: 0.018934962723181847.\n",
      "Step: 3465000. Mean Reward: -8.207587167540606. Std of Reward: 0.023001388606966194.\n",
      "Step: 3472500. Mean Reward: -6.545830969213932. Std of Reward: 5.231965656684498.\n",
      "Saved Model\n",
      "Step: 3482500. Mean Reward: -8.193868224967705. Std of Reward: 0.02573533023495443.\n",
      "Step: 3490000. Mean Reward: -8.191753276717318. Std of Reward: 0.025154631286632596.\n",
      "Step: 3497500. Mean Reward: -8.197398685226663. Std of Reward: 0.029201134277760568.\n",
      "Saved Model\n",
      "Step: 3507500. Mean Reward: -8.178141406860803. Std of Reward: 0.01944888020988234.\n",
      "Step: 3515000. Mean Reward: -8.192299387483706. Std of Reward: 0.03022291578571756.\n",
      "Step: 3522500. Mean Reward: -8.182060358542675. Std of Reward: 0.02383891047113319.\n",
      "Saved Model\n",
      "Step: 3530000. Mean Reward: -8.194123995025386. Std of Reward: 0.02122834410021537.\n",
      "Step: 3540000. Mean Reward: -8.207373087092588. Std of Reward: 0.020438561461811104.\n",
      "Step: 3547500. Mean Reward: -8.190408403613311. Std of Reward: 0.023074777416663014.\n",
      "Saved Model\n",
      "Step: 3555000. Mean Reward: -8.190650943370477. Std of Reward: 0.023432730926422573.\n",
      "Step: 3562500. Mean Reward: -8.205008204167457. Std of Reward: 0.02275600214015649.\n",
      "Step: 3572500. Mean Reward: -7.547087158064503. Std of Reward: 2.06136157731121.\n",
      "Saved Model\n",
      "Step: 3580000. Mean Reward: -8.204953381792421. Std of Reward: 0.030190053988803027.\n",
      "Step: 3587500. Mean Reward: -8.18478199198291. Std of Reward: 0.022776770985009097.\n",
      "Step: 3597500. Mean Reward: -8.205172654792921. Std of Reward: 0.015461390796655304.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 3605000. Mean Reward: -8.191451684827955. Std of Reward: 0.0251542181872885.\n",
      "Step: 3612500. Mean Reward: -8.200062524399295. Std of Reward: 0.016902044024247092.\n",
      "Step: 3620000. Mean Reward: -8.200629159747756. Std of Reward: 0.021544598166888928.\n",
      "Saved Model\n",
      "Step: 3630000. Mean Reward: -8.184345857544931. Std of Reward: 0.022217100825529305.\n",
      "Step: 3637500. Mean Reward: -8.185221254382972. Std of Reward: 0.022509149619746815.\n",
      "Step: 3645000. Mean Reward: -8.188151704587801. Std of Reward: 0.02897740257558819.\n",
      "Saved Model\n",
      "Step: 3655000. Mean Reward: -8.187356950668267. Std of Reward: 0.02234440689526263.\n",
      "Step: 3662500. Mean Reward: -8.198151055853405. Std of Reward: 0.024652917386424025.\n",
      "Step: 3670000. Mean Reward: -8.193132745214575. Std of Reward: 0.027259822923556328.\n",
      "Saved Model\n",
      "Step: 3677500. Mean Reward: -8.183471147570332. Std of Reward: 0.01842235308562292.\n",
      "Step: 3687500. Mean Reward: -8.195764581973565. Std of Reward: 0.0246328299552228.\n",
      "Step: 3695000. Mean Reward: -8.19215679342874. Std of Reward: 0.014346463751242638.\n",
      "Saved Model\n",
      "Step: 3702500. Mean Reward: -8.19227466759273. Std of Reward: 0.021728294820334403.\n",
      "Step: 3710000. Mean Reward: -8.196984445189099. Std of Reward: 0.028545612153463646.\n",
      "Step: 3720000. Mean Reward: -8.194755517789163. Std of Reward: 0.018831274503570288.\n",
      "Saved Model\n",
      "Step: 3727500. Mean Reward: -8.18777702516173. Std of Reward: 0.021647603843721084.\n",
      "Step: 3735000. Mean Reward: -8.199255906785346. Std of Reward: 0.02337745912062069.\n",
      "Step: 3745000. Mean Reward: -8.196679498275397. Std of Reward: 0.02864953681132864.\n",
      "Saved Model\n",
      "Step: 3752500. Mean Reward: -8.194962541099823. Std of Reward: 0.026881709142411586.\n",
      "Step: 3760000. Mean Reward: -8.181893502686473. Std of Reward: 0.02333720587868537.\n",
      "Step: 3767500. Mean Reward: -8.189608117476501. Std of Reward: 0.022761282557257182.\n",
      "Saved Model\n",
      "Step: 3777500. Mean Reward: -8.19374893339487. Std of Reward: 0.025954570618173262.\n",
      "Step: 3785000. Mean Reward: -8.188616029422121. Std of Reward: 0.019506277443993053.\n",
      "Step: 3792500. Mean Reward: -8.192026635733722. Std of Reward: 0.01916726032452325.\n",
      "Saved Model\n",
      "Step: 3802500. Mean Reward: -8.182724687186894. Std of Reward: 0.01834525446837942.\n",
      "Step: 3810000. Mean Reward: -8.19387279643253. Std of Reward: 0.02419016823814196.\n",
      "Step: 3817500. Mean Reward: -8.193021623249367. Std of Reward: 0.022540828674161215.\n",
      "Step: 3825000. Mean Reward: -8.20727031838248. Std of Reward: 0.02163346961513272.\n",
      "Saved Model\n",
      "Step: 3835000. Mean Reward: -8.195906564234157. Std of Reward: 0.02259235043395166.\n",
      "Step: 3842500. Mean Reward: -8.199014589206154. Std of Reward: 0.021804543572221523.\n",
      "Step: 3850000. Mean Reward: -8.189165880429584. Std of Reward: 0.019240840083296504.\n",
      "Saved Model\n",
      "Step: 3857500. Mean Reward: -8.182992898911266. Std of Reward: 0.02010741346626133.\n",
      "Step: 3867500. Mean Reward: -8.182693483633514. Std of Reward: 0.021765993795673587.\n",
      "Step: 3875000. Mean Reward: -8.193164369794053. Std of Reward: 0.025224863900804737.\n",
      "Saved Model\n",
      "Step: 3882500. Mean Reward: -8.196320955582816. Std of Reward: 0.023307333254678222.\n",
      "Step: 3892500. Mean Reward: -8.196967393573974. Std of Reward: 0.02615967923525444.\n",
      "Step: 3900000. Mean Reward: -8.18451687056374. Std of Reward: 0.021622403176360345.\n",
      "Saved Model\n",
      "Step: 3907500. Mean Reward: -8.199726640692518. Std of Reward: 0.021692093155507887.\n",
      "Step: 3915000. Mean Reward: -8.188499910186144. Std of Reward: 0.016249630518429786.\n",
      "Step: 3925000. Mean Reward: -8.19839210641215. Std of Reward: 0.02408385148198101.\n",
      "Saved Model\n",
      "Step: 3932500. Mean Reward: -8.177900735496156. Std of Reward: 0.017585261120519474.\n",
      "Step: 3940000. Mean Reward: -8.182696992425445. Std of Reward: 0.018328046369265195.\n",
      "Step: 3950000. Mean Reward: -8.196134263791297. Std of Reward: 0.020364287162504227.\n",
      "Saved Model\n",
      "Step: 3957500. Mean Reward: -8.187310144279035. Std of Reward: 0.02189126729113946.\n",
      "Step: 3965000. Mean Reward: -8.204150506586469. Std of Reward: 0.02365065775503467.\n",
      "Step: 3972500. Mean Reward: -8.200187227000281. Std of Reward: 0.020625685980042812.\n",
      "Saved Model\n",
      "Step: 3982500. Mean Reward: -8.187608377038808. Std of Reward: 0.025332327486440175.\n",
      "Step: 3990000. Mean Reward: -8.196758233575018. Std of Reward: 0.023579986341616645.\n",
      "Step: 3997500. Mean Reward: -8.182614022854215. Std of Reward: 0.020912867561217.\n",
      "Saved Model\n",
      "Step: 4005000. Mean Reward: -8.192913209520295. Std of Reward: 0.026013910410245072.\n",
      "Step: 4015000. Mean Reward: -8.198520318451527. Std of Reward: 0.024918655141612806.\n",
      "Step: 4022500. Mean Reward: -8.203236390612684. Std of Reward: 0.021097034013775575.\n",
      "Saved Model\n",
      "Step: 4030000. Mean Reward: -8.202001153314498. Std of Reward: 0.02540347658991918.\n",
      "Step: 4040000. Mean Reward: -8.186470071469333. Std of Reward: 0.01898662653400443.\n",
      "Step: 4047500. Mean Reward: -8.189591676238821. Std of Reward: 0.02456923286608544.\n",
      "Saved Model\n",
      "Step: 4055000. Mean Reward: -8.207024918936913. Std of Reward: 0.023746602456329164.\n",
      "Step: 4062500. Mean Reward: -8.18808746491833. Std of Reward: 0.016412566647964182.\n",
      "Step: 4072500. Mean Reward: -8.204014665076283. Std of Reward: 0.016961986454252908.\n",
      "Saved Model\n",
      "Step: 4080000. Mean Reward: -8.193539927348137. Std of Reward: 0.025622311670035328.\n",
      "Step: 4087500. Mean Reward: -8.185438924407654. Std of Reward: 0.025588603401219656.\n",
      "Step: 4097500. Mean Reward: -8.195584263435967. Std of Reward: 0.02361172419799812.\n",
      "Saved Model\n",
      "Step: 4105000. Mean Reward: -8.183092312600838. Std of Reward: 0.02734366894401198.\n",
      "Step: 4112500. Mean Reward: -8.21001141046938. Std of Reward: 0.029930406763883786.\n",
      "Step: 4120000. Mean Reward: -8.192155728520367. Std of Reward: 0.035334678434585785.\n",
      "Saved Model\n",
      "Step: 4130000. Mean Reward: -8.212537045187098. Std of Reward: 0.02595951117596489.\n",
      "Step: 4137500. Mean Reward: -8.185114559761999. Std of Reward: 0.026397456004164543.\n",
      "Step: 4145000. Mean Reward: -6.540812215433696. Std of Reward: 5.230412355345276.\n",
      "Saved Model\n",
      "Step: 4152500. Mean Reward: -8.194260873613986. Std of Reward: 0.03373792721765604.\n",
      "Step: 4162500. Mean Reward: -8.18178207890087. Std of Reward: 0.027099979581676956.\n",
      "Step: 4170000. Mean Reward: -8.18943253040785. Std of Reward: 0.024968496933385785.\n",
      "Saved Model\n",
      "Step: 4177500. Mean Reward: -8.193972172993444. Std of Reward: 0.027436579194359767.\n",
      "Step: 4187500. Mean Reward: -8.207219967221429. Std of Reward: 0.029274526991165725.\n",
      "Step: 4195000. Mean Reward: -7.52937658439043. Std of Reward: 2.062348087768224.\n",
      "Saved Model\n",
      "Step: 4202500. Mean Reward: -8.18954441998999. Std of Reward: 0.028051556617836274.\n",
      "Step: 4210000. Mean Reward: -8.20099033646515. Std of Reward: 0.018193245051829787.\n",
      "Step: 4220000. Mean Reward: -7.523705586395925. Std of Reward: 2.0627506332357877.\n",
      "Saved Model\n",
      "Step: 4227500. Mean Reward: -8.191422240554555. Std of Reward: 0.026721320949808145.\n",
      "Step: 4235000. Mean Reward: -8.180452615711586. Std of Reward: 0.02257356636191192.\n",
      "Step: 4245000. Mean Reward: -8.19338630912288. Std of Reward: 0.027930024156583712.\n",
      "Saved Model\n",
      "Step: 4252500. Mean Reward: -8.187802507474158. Std of Reward: 0.024755862239424914.\n",
      "Step: 4260000. Mean Reward: -7.547183917498473. Std of Reward: 2.0627432071834293.\n",
      "Step: 4267500. Mean Reward: -8.190873782496453. Std of Reward: 0.02870816006831213.\n",
      "Saved Model\n",
      "Step: 4277500. Mean Reward: -8.196759187971658. Std of Reward: 0.02322023670594441.\n",
      "Step: 4285000. Mean Reward: -8.1928911228946. Std of Reward: 0.023267492672828214.\n",
      "Step: 4292500. Mean Reward: -8.20622326098934. Std of Reward: 0.026154615118365636.\n",
      "Step: 4300000. Mean Reward: -8.194606363130735. Std of Reward: 0.025735368807435963.\n",
      "Saved Model\n",
      "Step: 4310000. Mean Reward: -8.191795808829545. Std of Reward: 0.018130069283916632.\n",
      "Step: 4317500. Mean Reward: -8.189853898908652. Std of Reward: 0.028847822620438866.\n",
      "Step: 4325000. Mean Reward: -8.190284082025489. Std of Reward: 0.024031838579552517.\n",
      "Saved Model\n",
      "Step: 4335000. Mean Reward: -8.186451112500128. Std of Reward: 0.0232871407720341.\n",
      "Step: 4342500. Mean Reward: -8.200057908720165. Std of Reward: 0.025440676700837293.\n",
      "Step: 4350000. Mean Reward: -8.191318475175658. Std of Reward: 0.026533605673449555.\n",
      "Saved Model\n",
      "Step: 4357500. Mean Reward: -8.198841134477576. Std of Reward: 0.02626360238404728.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4367500. Mean Reward: -8.18970607906699. Std of Reward: 0.023255355452998904.\n",
      "Step: 4375000. Mean Reward: -8.207856599223888. Std of Reward: 0.021386295504218316.\n",
      "Saved Model\n",
      "Step: 4382500. Mean Reward: -8.189644471678971. Std of Reward: 0.022457081400609077.\n",
      "Step: 4392500. Mean Reward: -8.189349939101922. Std of Reward: 0.026632917645919477.\n",
      "Step: 4400000. Mean Reward: -5.160234467086824. Std of Reward: 6.77906828021954.\n",
      "Saved Model\n",
      "Step: 4407500. Mean Reward: -8.19246318678474. Std of Reward: 0.024928494051816123.\n",
      "Step: 4415000. Mean Reward: -8.18530674147561. Std of Reward: 0.027613383157664685.\n",
      "Step: 4425000. Mean Reward: -8.190428773481663. Std of Reward: 0.022245010106848786.\n",
      "Saved Model\n",
      "Step: 4432500. Mean Reward: -8.187782746953424. Std of Reward: 0.019955009044010524.\n",
      "Step: 4440000. Mean Reward: -8.195397912582518. Std of Reward: 0.02377667080471225.\n",
      "Step: 4447500. Mean Reward: -8.199237977482616. Std of Reward: 0.020438168757597783.\n",
      "Saved Model\n",
      "Step: 4457500. Mean Reward: -8.202984772966348. Std of Reward: 0.018917071768286105.\n",
      "Step: 4465000. Mean Reward: -8.192448618023297. Std of Reward: 0.02391449887786712.\n",
      "Step: 4472500. Mean Reward: -8.198269579484178. Std of Reward: 0.028617581605828286.\n",
      "Saved Model\n",
      "Step: 4482500. Mean Reward: -8.20113643855767. Std of Reward: 0.024680982859567505.\n",
      "Step: 4490000. Mean Reward: -8.19355430840817. Std of Reward: 0.026178906679588772.\n",
      "Step: 4497500. Mean Reward: -8.194860536694895. Std of Reward: 0.02547261026859802.\n",
      "Saved Model\n",
      "Step: 4505000. Mean Reward: -8.201462748616862. Std of Reward: 0.01602361453298073.\n",
      "Step: 4515000. Mean Reward: -8.193939250322549. Std of Reward: 0.02587239556248071.\n",
      "Step: 4522500. Mean Reward: -8.19163150987211. Std of Reward: 0.02454034042762076.\n",
      "Saved Model\n",
      "Step: 4530000. Mean Reward: -8.199520303656682. Std of Reward: 0.022060735803689897.\n",
      "Step: 4540000. Mean Reward: -8.198191453523789. Std of Reward: 0.023936145798287504.\n",
      "Step: 4547500. Mean Reward: -8.190834619362182. Std of Reward: 0.01996068105447515.\n",
      "Saved Model\n",
      "Step: 4555000. Mean Reward: -7.542039784093259. Std of Reward: 2.068579149522837.\n",
      "Step: 4562500. Mean Reward: -8.189743048241656. Std of Reward: 0.02452001276645078.\n",
      "Step: 4572500. Mean Reward: -8.194185182465558. Std of Reward: 0.027821293350518698.\n",
      "Saved Model\n",
      "Step: 4580000. Mean Reward: -8.202005846403413. Std of Reward: 0.03291519165768956.\n",
      "Step: 4587500. Mean Reward: -8.197167131308728. Std of Reward: 0.0227667651036205.\n",
      "Step: 4597500. Mean Reward: -8.203502389811803. Std of Reward: 0.021590113100925082.\n",
      "Saved Model\n",
      "Step: 4605000. Mean Reward: -8.200276948864616. Std of Reward: 0.022991048654413353.\n",
      "Step: 4612500. Mean Reward: -8.208201638979471. Std of Reward: 0.020303477627357507.\n",
      "Step: 4620000. Mean Reward: -8.189826585567673. Std of Reward: 0.022249900013340367.\n",
      "Saved Model\n",
      "Step: 4630000. Mean Reward: -8.197006379904513. Std of Reward: 0.0216233420704152.\n",
      "Step: 4637500. Mean Reward: -8.193399889353152. Std of Reward: 0.023489740647620102.\n",
      "Step: 4645000. Mean Reward: -8.188397982106505. Std of Reward: 0.021609240346884336.\n",
      "Saved Model\n",
      "Step: 4652500. Mean Reward: -8.19792793504936. Std of Reward: 0.02878792056010473.\n",
      "Step: 4662500. Mean Reward: -8.193942720375073. Std of Reward: 0.023277134970773248.\n",
      "Step: 4670000. Mean Reward: -8.195427019111703. Std of Reward: 0.023428287593686273.\n",
      "Saved Model\n",
      "Step: 4677500. Mean Reward: -6.542808237425107. Std of Reward: 5.231023105579002.\n",
      "Step: 4687500. Mean Reward: -8.20841763191218. Std of Reward: 0.024320682196714405.\n",
      "Step: 4695000. Mean Reward: -8.198565599672559. Std of Reward: 0.01911479914768841.\n",
      "Saved Model\n",
      "Step: 4702500. Mean Reward: -8.183088762072119. Std of Reward: 0.021023761932454373.\n",
      "Step: 4710000. Mean Reward: -8.19564808069162. Std of Reward: 0.027078096756882306.\n",
      "Step: 4720000. Mean Reward: -7.532727225716644. Std of Reward: 2.065389994867601.\n",
      "Saved Model\n",
      "Step: 4727500. Mean Reward: -8.18558569366608. Std of Reward: 0.02147104692734169.\n",
      "Step: 4735000. Mean Reward: -8.185367471628513. Std of Reward: 0.025512481048565832.\n",
      "Step: 4745000. Mean Reward: -6.5464948992104155. Std of Reward: 5.2322060306108895.\n",
      "Saved Model\n",
      "Step: 4752500. Mean Reward: -8.191727720741236. Std of Reward: 0.023896086021303088.\n",
      "Step: 4760000. Mean Reward: -8.202002298004874. Std of Reward: 0.026170604073301972.\n",
      "Step: 4767500. Mean Reward: -8.184808351314823. Std of Reward: 0.025397776054339117.\n",
      "Saved Model\n",
      "Step: 4777500. Mean Reward: -8.182881242350266. Std of Reward: 0.022235274098144936.\n",
      "Step: 4785000. Mean Reward: -8.195578538763758. Std of Reward: 0.02342884547997804.\n",
      "Step: 4792500. Mean Reward: -8.201709253124765. Std of Reward: 0.02464696311003824.\n",
      "Step: 4800000. Mean Reward: -8.202463533096486. Std of Reward: 0.027183013410559616.\n",
      "Saved Model\n",
      "Step: 4810000. Mean Reward: -8.187774545349953. Std of Reward: 0.02398104789808122.\n",
      "Step: 4817500. Mean Reward: -8.200970269600635. Std of Reward: 0.019210658877572166.\n",
      "Step: 4825000. Mean Reward: -8.18366745227436. Std of Reward: 0.02118674603577622.\n",
      "Saved Model\n",
      "Step: 4835000. Mean Reward: -8.190653726719528. Std of Reward: 0.018456565645743207.\n",
      "Step: 4842500. Mean Reward: -8.188058282468841. Std of Reward: 0.021146753283798177.\n",
      "Step: 4850000. Mean Reward: -8.203018190216032. Std of Reward: 0.0267153083197893.\n",
      "Saved Model\n",
      "Step: 4857500. Mean Reward: -8.188118173489498. Std of Reward: 0.02224537558218726.\n",
      "Step: 4867500. Mean Reward: -7.543670878876318. Std of Reward: 2.0691183689279358.\n",
      "Step: 4875000. Mean Reward: -8.19625950441401. Std of Reward: 0.023878054201608673.\n",
      "Saved Model\n",
      "Step: 4882500. Mean Reward: -8.195287858496648. Std of Reward: 0.02211303526509705.\n",
      "Step: 4892500. Mean Reward: -8.197504464760629. Std of Reward: 0.02585365535612948.\n",
      "Step: 4900000. Mean Reward: -8.185164987158675. Std of Reward: 0.019993395869022268.\n",
      "Saved Model\n",
      "Step: 4907500. Mean Reward: -8.189359132274433. Std of Reward: 0.020457280575177986.\n",
      "Step: 4915000. Mean Reward: -8.177992517137477. Std of Reward: 0.025110296704491903.\n",
      "Step: 4925000. Mean Reward: -8.215932351397877. Std of Reward: 0.01766413481721762.\n",
      "Saved Model\n",
      "Step: 4932500. Mean Reward: -8.208336755127723. Std of Reward: 0.01603484766490904.\n",
      "Step: 4940000. Mean Reward: -8.195346642862718. Std of Reward: 0.02209136503982793.\n",
      "Step: 4947500. Mean Reward: -8.197329102850869. Std of Reward: 0.02266679550329476.\n",
      "Saved Model\n",
      "Step: 4957500. Mean Reward: -8.19721660766819. Std of Reward: 0.02212411295306362.\n",
      "Step: 4965000. Mean Reward: -8.189582711669434. Std of Reward: 0.022175528005719154.\n",
      "Step: 4972500. Mean Reward: -8.190794259604067. Std of Reward: 0.025689054617414598.\n",
      "Saved Model\n",
      "Step: 4982500. Mean Reward: -8.195492059216567. Std of Reward: 0.022523463912331558.\n",
      "Step: 4990000. Mean Reward: -8.197635728471163. Std of Reward: 0.02799447049306687.\n",
      "Step: 4997500. Mean Reward: -8.195498963911527. Std of Reward: 0.027254496279419206.\n",
      "Saved Model\n",
      "Step: 5005000. Mean Reward: -8.19324100660807. Std of Reward: 0.022429949615857824.\n",
      "Step: 5015000. Mean Reward: -8.184640199326441. Std of Reward: 0.012413715474810927.\n",
      "Step: 5022500. Mean Reward: -8.204735872143088. Std of Reward: 0.017563982476929547.\n",
      "Saved Model\n",
      "Step: 5030000. Mean Reward: -8.19108123968745. Std of Reward: 0.022633237537836713.\n",
      "Step: 5040000. Mean Reward: -8.2060880294604. Std of Reward: 0.022787230057428647.\n",
      "Step: 5047500. Mean Reward: -8.21099231854068. Std of Reward: 0.01481793745677133.\n",
      "Saved Model\n",
      "Step: 5055000. Mean Reward: -8.188494454911284. Std of Reward: 0.021892536879875493.\n",
      "Step: 5062500. Mean Reward: -8.185955850480674. Std of Reward: 0.025549602662689033.\n",
      "Step: 5072500. Mean Reward: -8.201664201261151. Std of Reward: 0.027740143871060684.\n",
      "Saved Model\n",
      "Step: 5080000. Mean Reward: -8.189382173253145. Std of Reward: 0.02199312776304065.\n",
      "Step: 5087500. Mean Reward: -8.208138886885575. Std of Reward: 0.016495559329357413.\n",
      "Step: 5095000. Mean Reward: -8.190302964829451. Std of Reward: 0.0225501154933554.\n",
      "Saved Model\n",
      "Step: 5105000. Mean Reward: -8.19380966349206. Std of Reward: 0.030022915764215192.\n",
      "Step: 5112500. Mean Reward: -8.202141763264706. Std of Reward: 0.026629183520201084.\n",
      "Step: 5120000. Mean Reward: -8.202939416471773. Std of Reward: 0.021343171628789355.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 5130000. Mean Reward: -8.193519098104606. Std of Reward: 0.024956952083114874.\n",
      "Step: 5137500. Mean Reward: -8.19318874495143. Std of Reward: 0.025498404902970975.\n",
      "Step: 5145000. Mean Reward: -8.187063905493842. Std of Reward: 0.028303096040513887.\n",
      "Saved Model\n",
      "Step: 5152500. Mean Reward: -8.195677110413746. Std of Reward: 0.021979001196299922.\n",
      "Step: 5162500. Mean Reward: -8.19752720077299. Std of Reward: 0.029592333389381248.\n",
      "Step: 5170000. Mean Reward: -8.205126459285724. Std of Reward: 0.018498149879659156.\n",
      "Saved Model\n",
      "Step: 5177500. Mean Reward: -8.198354074668218. Std of Reward: 0.02386819259363952.\n",
      "Step: 5187500. Mean Reward: -8.18781044202606. Std of Reward: 0.02108800430573785.\n",
      "Step: 5195000. Mean Reward: -8.191152460072441. Std of Reward: 0.022581771764237313.\n",
      "Saved Model\n",
      "Step: 5202500. Mean Reward: -8.193308870736368. Std of Reward: 0.02444921389373064.\n",
      "Step: 5210000. Mean Reward: -8.197925722596564. Std of Reward: 0.01953960592863326.\n",
      "Step: 5220000. Mean Reward: -8.197344209634988. Std of Reward: 0.024112146639880466.\n",
      "Saved Model\n",
      "Step: 5227500. Mean Reward: -8.188163874544294. Std of Reward: 0.028778352925843177.\n",
      "Step: 5235000. Mean Reward: -8.186251376545695. Std of Reward: 0.01945324277046364.\n",
      "Step: 5242500. Mean Reward: -8.205103877231414. Std of Reward: 0.025558199956849714.\n",
      "Saved Model\n",
      "Step: 5252500. Mean Reward: -8.19899585946811. Std of Reward: 0.022959883652667823.\n",
      "Step: 5260000. Mean Reward: -8.192905810056178. Std of Reward: 0.0207781544461634.\n",
      "Step: 5267500. Mean Reward: -8.17980476677712. Std of Reward: 0.02274325436053769.\n",
      "Saved Model\n",
      "Step: 5277500. Mean Reward: -8.203034937383524. Std of Reward: 0.020465144544417523.\n",
      "Step: 5285000. Mean Reward: -8.187996675777512. Std of Reward: 0.021183225747873013.\n",
      "Step: 5292500. Mean Reward: -8.189364626073075. Std of Reward: 0.02506220446408993.\n",
      "Step: 5300000. Mean Reward: -8.186184161003021. Std of Reward: 0.016760517677542.\n",
      "Saved Model\n",
      "Step: 5310000. Mean Reward: -8.203427965042565. Std of Reward: 0.023354515425388505.\n",
      "Step: 5317500. Mean Reward: -8.192760469502376. Std of Reward: 0.024697531119043097.\n",
      "Step: 5325000. Mean Reward: -8.204471589994379. Std of Reward: 0.019777857592092593.\n",
      "Saved Model\n",
      "Step: 5335000. Mean Reward: -8.203879853898647. Std of Reward: 0.019785448704128294.\n",
      "Step: 5342500. Mean Reward: -8.195732538247478. Std of Reward: 0.024780405977897352.\n",
      "Step: 5350000. Mean Reward: -8.1932019443288. Std of Reward: 0.02283500831300415.\n",
      "Saved Model\n",
      "Step: 5357500. Mean Reward: -8.194152070849363. Std of Reward: 0.02559687904366723.\n",
      "Step: 5367500. Mean Reward: -8.201021081649804. Std of Reward: 0.02216465715684537.\n",
      "Step: 5375000. Mean Reward: -8.1960351957333. Std of Reward: 0.02799404206127855.\n",
      "Saved Model\n",
      "Step: 5382500. Mean Reward: -8.207930185152007. Std of Reward: 0.018293871326051026.\n",
      "Step: 5390000. Mean Reward: -8.207470437794811. Std of Reward: 0.019992165524962628.\n",
      "Step: 5400000. Mean Reward: -8.195026590177708. Std of Reward: 0.023222846621476907.\n",
      "Saved Model\n",
      "Step: 5407500. Mean Reward: -8.187025796757544. Std of Reward: 0.026223193398203642.\n",
      "Step: 5415000. Mean Reward: -8.204439203149843. Std of Reward: 0.0167586392070274.\n",
      "Step: 5425000. Mean Reward: -8.196761743083687. Std of Reward: 0.02565513129906642.\n",
      "Saved Model\n",
      "Step: 5432500. Mean Reward: -8.190302583657692. Std of Reward: 0.024979834828989703.\n",
      "Step: 5440000. Mean Reward: -8.197239877447675. Std of Reward: 0.02496963731677045.\n",
      "Step: 5447500. Mean Reward: -8.186958276749474. Std of Reward: 0.01913160710678723.\n",
      "Saved Model\n",
      "Step: 5457500. Mean Reward: -8.191970216492432. Std of Reward: 0.025824057565563316.\n",
      "Step: 5465000. Mean Reward: -8.196037294351907. Std of Reward: 0.022031984984776768.\n",
      "Step: 5472500. Mean Reward: -8.191191140759233. Std of Reward: 0.0213852964216657.\n",
      "Saved Model\n",
      "Step: 5482500. Mean Reward: -8.194551545778118. Std of Reward: 0.024163648014703047.\n",
      "Step: 5490000. Mean Reward: -8.195334474782607. Std of Reward: 0.024262778498530535.\n",
      "Step: 5497500. Mean Reward: -8.192142755599606. Std of Reward: 0.02384566356826965.\n",
      "Saved Model\n",
      "Step: 5505000. Mean Reward: -8.200132409788285. Std of Reward: 0.022617993763812762.\n",
      "Step: 5515000. Mean Reward: -8.200024492047007. Std of Reward: 0.023403256036615912.\n",
      "Step: 5522500. Mean Reward: -8.187481881986361. Std of Reward: 0.017610322453518755.\n",
      "Saved Model\n",
      "Step: 5530000. Mean Reward: -8.19507644800531. Std of Reward: 0.02444245412208633.\n",
      "Step: 5537500. Mean Reward: -8.201274110468173. Std of Reward: 0.023034752797884417.\n",
      "Step: 5547500. Mean Reward: -8.178074952657512. Std of Reward: 0.021084344376971532.\n",
      "Saved Model\n",
      "Step: 5555000. Mean Reward: -8.197508660900759. Std of Reward: 0.025060865304735405.\n",
      "Step: 5562500. Mean Reward: -8.200758325379045. Std of Reward: 0.021663044738866134.\n",
      "Step: 5572500. Mean Reward: -8.191571122762507. Std of Reward: 0.02517652080960493.\n",
      "Saved Model\n",
      "Step: 5580000. Mean Reward: -8.194792558394838. Std of Reward: 0.02356509224633299.\n",
      "Step: 5587500. Mean Reward: -8.198359376529655. Std of Reward: 0.021845156286313797.\n",
      "Step: 5595000. Mean Reward: -6.547225448653211. Std of Reward: 5.2324208885049295.\n",
      "Saved Model\n",
      "Step: 5605000. Mean Reward: -8.200823179717837. Std of Reward: 0.019841878817294167.\n",
      "Step: 5612500. Mean Reward: -8.195022470137578. Std of Reward: 0.021913362993046658.\n",
      "Step: 5620000. Mean Reward: -8.200578500570577. Std of Reward: 0.02834011834242249.\n",
      "Saved Model\n",
      "Step: 5630000. Mean Reward: -8.189468766931348. Std of Reward: 0.021169634783600805.\n",
      "Step: 5637500. Mean Reward: -8.195632058885327. Std of Reward: 0.023159046400583732.\n",
      "Step: 5645000. Mean Reward: -8.193759385659497. Std of Reward: 0.028173392141420305.\n",
      "Saved Model\n",
      "Step: 5652500. Mean Reward: -8.18211300035253. Std of Reward: 0.01758552254422019.\n",
      "Step: 5662500. Mean Reward: -8.186877595634702. Std of Reward: 0.02066704168384409.\n",
      "Step: 5670000. Mean Reward: -7.527310573324706. Std of Reward: 2.057381671347076.\n",
      "Saved Model\n",
      "Step: 5677500. Mean Reward: -8.184820497129301. Std of Reward: 0.026669079975782026.\n",
      "Step: 5685000. Mean Reward: -8.189804307776127. Std of Reward: 0.022465967617636754.\n",
      "Step: 5695000. Mean Reward: -8.1860114685676. Std of Reward: 0.024106163262328485.\n",
      "Saved Model\n",
      "Step: 5702500. Mean Reward: -8.191236650572469. Std of Reward: 0.028250643238047383.\n",
      "Step: 5710000. Mean Reward: -8.201623117409714. Std of Reward: 0.02661433416893135.\n",
      "Step: 5720000. Mean Reward: -8.212115899859166. Std of Reward: 0.01346574414612952.\n",
      "Saved Model\n",
      "Step: 5727500. Mean Reward: -8.203504563890544. Std of Reward: 0.028426110109176447.\n",
      "Step: 5735000. Mean Reward: -8.201906854136684. Std of Reward: 0.01951480952586008.\n",
      "Step: 5742500. Mean Reward: -8.192548409854059. Std of Reward: 0.02499756659896286.\n",
      "Saved Model\n",
      "Step: 5752500. Mean Reward: -8.190008470247992. Std of Reward: 0.022462996071036093.\n",
      "Step: 5760000. Mean Reward: -8.18863704847286. Std of Reward: 0.023532112739938656.\n",
      "Step: 5767500. Mean Reward: -8.204509774936131. Std of Reward: 0.023883375106829915.\n",
      "Saved Model\n",
      "Step: 5777500. Mean Reward: -8.177851983775042. Std of Reward: 0.02028624042071188.\n",
      "Step: 5785000. Mean Reward: -8.2084216323119. Std of Reward: 0.02079038206237526.\n",
      "Step: 5792500. Mean Reward: -8.200735208017116. Std of Reward: 0.019794328630292964.\n",
      "Step: 5800000. Mean Reward: -8.208196031016303. Std of Reward: 0.018308426230309843.\n",
      "Saved Model\n",
      "Step: 5810000. Mean Reward: -8.195849000425687. Std of Reward: 0.026144128446689816.\n",
      "Step: 5817500. Mean Reward: -8.194002877707417. Std of Reward: 0.028490626763487167.\n",
      "Step: 5825000. Mean Reward: -8.197969590538298. Std of Reward: 0.02611730198933814.\n",
      "Saved Model\n",
      "Step: 5832500. Mean Reward: -8.203236466622885. Std of Reward: 0.026051435800860796.\n",
      "Step: 5842500. Mean Reward: -8.1880746094079. Std of Reward: 0.023203954525294775.\n",
      "Step: 5850000. Mean Reward: -8.190317841992552. Std of Reward: 0.023713458512602686.\n",
      "Saved Model\n",
      "Step: 5857500. Mean Reward: -8.187531625288681. Std of Reward: 0.024159741774716702.\n",
      "Step: 5867500. Mean Reward: -8.193976480123752. Std of Reward: 0.022990446374158948.\n",
      "Step: 5875000. Mean Reward: -8.19368396915754. Std of Reward: 0.023459316589801753.\n",
      "Saved Model\n",
      "Step: 5882500. Mean Reward: -8.197630998029402. Std of Reward: 0.024387391314185616.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5890000. Mean Reward: -8.201140023669005. Std of Reward: 0.025423824505954772.\n",
      "Step: 5900000. Mean Reward: -8.182146493541298. Std of Reward: 0.023270437400185303.\n",
      "Saved Model\n",
      "Step: 5907500. Mean Reward: -8.188368112115274. Std of Reward: 0.028952559605040395.\n",
      "Step: 5915000. Mean Reward: -8.19733360401176. Std of Reward: 0.02159179555147837.\n",
      "Step: 5925000. Mean Reward: -8.191697312784676. Std of Reward: 0.022512698088566808.\n",
      "Saved Model\n",
      "Step: 5932500. Mean Reward: -8.19767696543384. Std of Reward: 0.024738045530704633.\n",
      "Step: 5940000. Mean Reward: -8.183834649763263. Std of Reward: 0.02342451287036885.\n",
      "Step: 5947500. Mean Reward: -8.203791581703225. Std of Reward: 0.018620777407804705.\n",
      "Saved Model\n",
      "Step: 5957500. Mean Reward: -8.196718026521356. Std of Reward: 0.025261928808606907.\n",
      "Step: 5965000. Mean Reward: -8.184285241674012. Std of Reward: 0.024804186146202403.\n",
      "Step: 5972500. Mean Reward: -8.195201188636407. Std of Reward: 0.026755018368800083.\n",
      "Saved Model\n",
      "Step: 5980000. Mean Reward: -8.196157227876974. Std of Reward: 0.022678785966522536.\n",
      "Step: 5990000. Mean Reward: -8.193538705594529. Std of Reward: 0.022576021146353185.\n",
      "Step: 5997500. Mean Reward: -8.188348313853915. Std of Reward: 0.021831413819616084.\n",
      "Saved Model\n",
      "Step: 6005000. Mean Reward: -8.186465646529983. Std of Reward: 0.02497118893709516.\n",
      "Step: 6015000. Mean Reward: -8.183463250995075. Std of Reward: 0.022490314689910224.\n",
      "Step: 6022500. Mean Reward: -8.20260383775736. Std of Reward: 0.024326294049353554.\n",
      "Saved Model\n",
      "Step: 6030000. Mean Reward: -8.207214471345802. Std of Reward: 0.029266113210343576.\n",
      "Step: 6037500. Mean Reward: -8.195338060773924. Std of Reward: 0.0264967684086252.\n",
      "Step: 6047500. Mean Reward: -8.185893595197403. Std of Reward: 0.02603298095833663.\n",
      "Saved Model\n",
      "Step: 6055000. Mean Reward: -8.188888208769548. Std of Reward: 0.020675420886960436.\n",
      "Step: 6062500. Mean Reward: -8.201613580379435. Std of Reward: 0.023247550242351908.\n",
      "Step: 6072500. Mean Reward: -8.191095277843049. Std of Reward: 0.022017635412014423.\n",
      "Saved Model\n",
      "Step: 6080000. Mean Reward: -8.192162019696163. Std of Reward: 0.02645411643912641.\n",
      "Step: 6087500. Mean Reward: -8.201013681032714. Std of Reward: 0.02173496151795398.\n",
      "Step: 6095000. Mean Reward: -8.17273853465153. Std of Reward: 0.008578840899877817.\n",
      "Saved Model\n",
      "Step: 6105000. Mean Reward: -8.1910034959806. Std of Reward: 0.026460807373089982.\n",
      "Step: 6112500. Mean Reward: -8.189220315948884. Std of Reward: 0.02173642828232312.\n",
      "Step: 6120000. Mean Reward: -8.198121415786915. Std of Reward: 0.024555094053013572.\n",
      "Saved Model\n",
      "Step: 6127500. Mean Reward: -8.189666368188421. Std of Reward: 0.0266544979651005.\n",
      "Step: 6137500. Mean Reward: -8.18666813140297. Std of Reward: 0.026883883894248315.\n",
      "Step: 6145000. Mean Reward: -8.19093570903317. Std of Reward: 0.03044319834849384.\n",
      "Saved Model\n",
      "Step: 6152500. Mean Reward: -8.186725351152093. Std of Reward: 0.019218347470337436.\n",
      "Step: 6162500. Mean Reward: -8.191205102663982. Std of Reward: 0.025291503370473496.\n",
      "Step: 6170000. Mean Reward: -8.195462800574948. Std of Reward: 0.023771298840161662.\n",
      "Saved Model\n",
      "Step: 6177500. Mean Reward: -8.19029834934761. Std of Reward: 0.023766204147082234.\n",
      "Step: 6185000. Mean Reward: -8.203591882626293. Std of Reward: 0.024416896097354708.\n",
      "Step: 6195000. Mean Reward: -8.195014955544025. Std of Reward: 0.024650757978771858.\n",
      "Saved Model\n",
      "Step: 6202500. Mean Reward: -8.19914203813725. Std of Reward: 0.02223553339699875.\n",
      "Step: 6210000. Mean Reward: -8.198056375270717. Std of Reward: 0.025223493701504694.\n",
      "Step: 6220000. Mean Reward: -8.202469903721703. Std of Reward: 0.01794483230179482.\n",
      "Saved Model\n",
      "Step: 6227500. Mean Reward: -8.198233415151526. Std of Reward: 0.026573052914873795.\n",
      "Step: 6235000. Mean Reward: -8.186574594226034. Std of Reward: 0.01723933229539056.\n",
      "Step: 6242500. Mean Reward: -8.197557221998657. Std of Reward: 0.02322983012413746.\n",
      "Saved Model\n",
      "Step: 6252500. Mean Reward: -8.207562829654213. Std of Reward: 0.019375740142145156.\n",
      "Step: 6260000. Mean Reward: -8.186476670727204. Std of Reward: 0.021983837078938074.\n",
      "Step: 6267500. Mean Reward: -8.18261531976241. Std of Reward: 0.019653671287871127.\n",
      "Step: 6275000. Mean Reward: -8.183474542341422. Std of Reward: 0.01951741276705159.\n",
      "Saved Model\n",
      "Step: 6285000. Mean Reward: -8.194730187951688. Std of Reward: 0.024205650021245452.\n",
      "Step: 6292500. Mean Reward: -8.185875970596726. Std of Reward: 0.023937766666637286.\n",
      "Step: 6300000. Mean Reward: -8.207666551106506. Std of Reward: 0.019938103232379243.\n",
      "Saved Model\n",
      "Step: 6310000. Mean Reward: -8.190325585844615. Std of Reward: 0.02600866609345817.\n",
      "Step: 6317500. Mean Reward: -8.202175714072563. Std of Reward: 0.026841612121302833.\n",
      "Step: 6325000. Mean Reward: -8.210413094740138. Std of Reward: 0.021785804230515217.\n",
      "Saved Model\n",
      "Step: 6332500. Mean Reward: -8.202410051133953. Std of Reward: 0.025769468840207078.\n",
      "Step: 6342500. Mean Reward: -8.194203149512095. Std of Reward: 0.023821863765755066.\n",
      "Step: 6350000. Mean Reward: -8.193977853389606. Std of Reward: 0.022254687333280027.\n",
      "Saved Model\n",
      "Step: 6357500. Mean Reward: -8.20944942598396. Std of Reward: 0.018372975045534874.\n",
      "Step: 6367500. Mean Reward: -8.187093469641166. Std of Reward: 0.02389851619801707.\n",
      "Step: 6375000. Mean Reward: -8.19304435958517. Std of Reward: 0.021565680004225075.\n",
      "Saved Model\n",
      "Step: 6382500. Mean Reward: -8.188121187803159. Std of Reward: 0.02274298933413675.\n",
      "Step: 6390000. Mean Reward: -8.196422464697832. Std of Reward: 0.032517994495704215.\n",
      "Step: 6400000. Mean Reward: -8.192176515509395. Std of Reward: 0.019100213383654092.\n",
      "Saved Model\n",
      "Step: 6407500. Mean Reward: -8.188543244876092. Std of Reward: 0.024767427160222743.\n",
      "Step: 6415000. Mean Reward: -8.18033168979224. Std of Reward: 0.01975857449712709.\n",
      "Step: 6422500. Mean Reward: -8.191366655228252. Std of Reward: 0.023656736670778167.\n",
      "Saved Model\n",
      "Step: 6432500. Mean Reward: -8.199808771286106. Std of Reward: 0.026673831709575734.\n",
      "Step: 6440000. Mean Reward: -8.179541590025956. Std of Reward: 0.025734479173187268.\n",
      "Step: 6447500. Mean Reward: -8.200714304212704. Std of Reward: 0.019834595338601806.\n",
      "Saved Model\n",
      "Step: 6457500. Mean Reward: -8.188957178153764. Std of Reward: 0.028603299972454074.\n",
      "Step: 6465000. Mean Reward: -8.180616990986232. Std of Reward: 0.020970512674872176.\n",
      "Step: 6472500. Mean Reward: -8.200555650556632. Std of Reward: 0.025794900329822085.\n",
      "Saved Model\n",
      "Step: 6480000. Mean Reward: -8.204220506285377. Std of Reward: 0.023318944310547058.\n",
      "Step: 6490000. Mean Reward: -8.202117387537992. Std of Reward: 0.021856668313960333.\n",
      "Step: 6497500. Mean Reward: -8.18307640247193. Std of Reward: 0.013858779186852351.\n",
      "Saved Model\n",
      "Step: 6505000. Mean Reward: -8.207577516512696. Std of Reward: 0.017220117741980977.\n",
      "Step: 6515000. Mean Reward: -8.193787919852843. Std of Reward: 0.02124555048365136.\n",
      "Step: 6522500. Mean Reward: -8.18870033425534. Std of Reward: 0.020099542262623043.\n",
      "Saved Model\n",
      "Step: 6530000. Mean Reward: -8.178964387707103. Std of Reward: 0.025204048737534675.\n",
      "Step: 6537500. Mean Reward: -8.186820184404684. Std of Reward: 0.021723702985003538.\n",
      "Step: 6540000. Mean Reward: -2.213057179918978. Std of Reward: 0.0.\n",
      "Step: 6547500. Mean Reward: -8.085061268668063. Std of Reward: 0.3513987036014124.\n",
      "Saved Model\n",
      "Step: 6555000. Mean Reward: -8.201010323812431. Std of Reward: 0.024294239364790686.\n",
      "Step: 6562500. Mean Reward: -8.193482934692081. Std of Reward: 0.023350173788163554.\n",
      "Step: 6572500. Mean Reward: -8.181424447771475. Std of Reward: 0.019710810919071745.\n",
      "Saved Model\n",
      "Step: 6580000. Mean Reward: -8.192242814512031. Std of Reward: 0.02144723281986062.\n",
      "Step: 6587500. Mean Reward: -7.544018537851409. Std of Reward: 2.0634486928029134.\n",
      "Step: 6595000. Mean Reward: -8.195945056183197. Std of Reward: 0.024691225448561043.\n",
      "Saved Model\n",
      "Step: 6605000. Mean Reward: -8.190207978739142. Std of Reward: 0.023758759206223754.\n",
      "Step: 6612500. Mean Reward: -8.198831598305102. Std of Reward: 0.030935947587025298.\n",
      "Step: 6620000. Mean Reward: -8.188905183487087. Std of Reward: 0.024163677841921624.\n",
      "Saved Model\n",
      "Step: 6627500. Mean Reward: -8.197079812670875. Std of Reward: 0.020560078480049357.\n",
      "Step: 6637500. Mean Reward: -6.5475701630247904. Std of Reward: 5.225376330899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6645000. Mean Reward: -8.19297998052583. Std of Reward: 0.01698084517587697.\n",
      "Saved Model\n",
      "Step: 6652500. Mean Reward: -8.183310586695082. Std of Reward: 0.013633147358928906.\n",
      "Step: 6662500. Mean Reward: -8.205978776500032. Std of Reward: 0.024864977344454312.\n",
      "Step: 6670000. Mean Reward: -8.196723939515957. Std of Reward: 0.024344279619340676.\n",
      "Saved Model\n",
      "Step: 6677500. Mean Reward: -6.539022219429044. Std of Reward: 5.229823100896806.\n",
      "Step: 6685000. Mean Reward: -8.210462080203298. Std of Reward: 0.018790191602975438.\n",
      "Step: 6695000. Mean Reward: -8.206144067309832. Std of Reward: 0.021384400610463327.\n",
      "Saved Model\n",
      "Step: 6702500. Mean Reward: -8.18916500257018. Std of Reward: 0.026966635280191224.\n",
      "Step: 6710000. Mean Reward: -8.204404870655043. Std of Reward: 0.024369792228822195.\n",
      "Step: 6720000. Mean Reward: -8.191230737184101. Std of Reward: 0.025516649698638205.\n",
      "Saved Model\n",
      "Step: 6727500. Mean Reward: -8.18841209583445. Std of Reward: 0.0283528057105001.\n",
      "Step: 6735000. Mean Reward: -8.19737716873162. Std of Reward: 0.029888489977251763.\n",
      "Step: 6742500. Mean Reward: -8.200863954521685. Std of Reward: 0.022101037391070073.\n",
      "Saved Model\n",
      "Step: 6752500. Mean Reward: -8.205924455615177. Std of Reward: 0.023638307434579352.\n",
      "Step: 6760000. Mean Reward: -8.20359100503755. Std of Reward: 0.022366163953534284.\n",
      "Step: 6767500. Mean Reward: -8.20581028135631. Std of Reward: 0.016727921446429598.\n",
      "Step: 6775000. Mean Reward: -8.189037667805271. Std of Reward: 0.026048814570611614.\n",
      "Saved Model\n",
      "Step: 6785000. Mean Reward: -8.19020584263902. Std of Reward: 0.017115154178242078.\n",
      "Step: 6792500. Mean Reward: -8.202143517838733. Std of Reward: 0.016549996025413734.\n",
      "Step: 6800000. Mean Reward: -8.19270866551154. Std of Reward: 0.023345407733995054.\n",
      "Saved Model\n",
      "Step: 6810000. Mean Reward: -8.187949716059252. Std of Reward: 0.019612673861933116.\n",
      "Step: 6817500. Mean Reward: -8.198818933183395. Std of Reward: 0.01815879498264018.\n",
      "Step: 6825000. Mean Reward: -8.183212282023693. Std of Reward: 0.018938130010798276.\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-89d5cb1cc86f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'actions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbuffer_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m# Perform gradient descent with experience buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mprocess_experiences\u001b[1;34m(self, info, time_horizon, gamma, lambd)\u001b[0m\n\u001b[0;32m    137\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'discounted_returns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'advantages'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value_estimates'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'actions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                     \u001b[0mappend_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_buffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_buffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                     \u001b[0mset_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_buffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_buffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\ppo\\history.py\u001b[0m in \u001b[0;36mappend_history\u001b[1;34m(global_buffer, local_buffer)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mglobal_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mglobal_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mglobal_buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep = 10000)\n",
    "\n",
    "print(\"Training started for {}\".format(env_name))\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        if load_model_from_diff_env:\n",
    "            print('Loading Model from different env with path {}'.format(load_model_env_path))\n",
    "            ckpt = tf.train.get_checkpoint_state(load_model_env_path)\n",
    "        else:\n",
    "            print('Loading Model from {}'.format(env_model_path))\n",
    "            ckpt = tf.train.get_checkpoint_state(env_model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(env_summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer =  Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.update_best(sess, saver=saver, model_path=env_model_path, steps=steps)\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=env_model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(env_model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v8.1cm/m1/Models/model_bests\\best_model.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v8.1cm/m1/Models/model_bests\\best_model.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n",
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v8.1cm/m1/Models\\model-6825000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v8.1cm/m1/Models\\model-6825000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 5 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 5 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 5 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_best_graph(trainer, env_model_path, model_name)\n",
    "export_graph(env_model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\ttarget_position_z -> 0.0\n",
      "\t\tnum_obstacles -> 1.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 22\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 3\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: Forward, Right, Left\n"
     ]
    }
   ],
   "source": [
    "port_save = random.randint(0,100)\n",
    "env = UnityEnvironment(file_name = project_path, base_port = 7013 + port_save)\n",
    "print(str(env))\n",
    "brain_name = env.brain_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The play session has started.\n",
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v8.0cm/m1/Models/model_bests\\best_model.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v8.0cm/m1/Models/model_bests\\best_model.cptk\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "unpack requires a buffer of 4 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-67cf656680fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_done\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, memory, value)\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"STEP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_brains\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"brain_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mn_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"agents\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"RECEIVED\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmessage_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmessage_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: unpack requires a buffer of 4 bytes"
     ]
    }
   ],
   "source": [
    "load_best = True;\n",
    "print(\"The play session has started.\")\n",
    "with tf.Session() as sess:\n",
    "    if load_best:\n",
    "        model_path_to_load = env_model_path + \"/model_bests\"\n",
    "        if not os.path.exists(model_path_to_load):\n",
    "            print(\"Model bests does not exist!\")\n",
    "            model_path_to_load = model_path\n",
    "    else:\n",
    "        model_path_to_load = env_model_path\n",
    "    ckpt = tf.train.get_checkpoint_state(model_path_to_load)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    steps = sess.run(ppo_model.global_step)\n",
    "    info = env.reset(train_mode=False)[brain_name]\n",
    "    trainer = Trainer( ppo_model, sess, info, is_continuous, use_observations, use_states, training=False)\n",
    "    \n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=False)[brain_name]\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps = 0, normalize = False)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
