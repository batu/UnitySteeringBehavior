{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 1th model\n",
      "{'batch_size': 64,\n",
      " 'beta': 0.001,\n",
      " 'buffer_size': 8192,\n",
      " 'curriculum_file': None,\n",
      " 'env_name': 'v0',\n",
      " 'epsilon': 0.1,\n",
      " 'gamma': 0.995,\n",
      " 'hidden_units': 32,\n",
      " 'lambd': 0.95,\n",
      " 'leaning_rate': 5e-05,\n",
      " 'max_steps': 50000000000.0,\n",
      " 'num_epoch': 3,\n",
      " 'projet_name': 'Steering',\n",
      " 'time_horizon': 2048}\n"
     ]
    }
   ],
   "source": [
    "#Change these two names when you work on a new project\n",
    "project_name = \"Steering\" # The sub-directory name for model and summary statistics or work on a new build \n",
    "env_name =     \"v0\" # Name of the training environment file.\n",
    "\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "\n",
    "model_number_to_load = 0 # which model to load\n",
    "                         #If this value is 0, load the last model in the given version.\n",
    "    \n",
    "load_best_model = False # You can either load the checkpoint with the best value or load the last checkpoint.\n",
    "\n",
    "max_steps = 5e10 # Set maximum number of steps to run environment.\n",
    "summary_freq = 2500 # Frequency at which to save training statistics.\n",
    "save_freq = 25000 # Frequency at which to save model.\n",
    "\n",
    "curriculum_file = None\n",
    "\n",
    "### Setting up the necesary directory structure and naming conventions\n",
    "project_path = \"Projects/{}/Builds/{}\".format(project_name, env_name)\n",
    "env_path = \"Projects/{}/{}\".format(project_name, env_name)\n",
    "\n",
    "if not os.path.exists(env_path):\n",
    "    os.makedirs(env_path)\n",
    "\n",
    "if not load_model:\n",
    "    model_count = len([i for i in os.listdir(env_path) if os.path.isdir(env_path)]) + 1\n",
    "    print(\"This is the {}th model\".format(model_count))\n",
    "elif load_model:\n",
    "    if model_number_to_load == 0:\n",
    "        model_number_to_load =  len([i for i in os.listdir(env_path) if os.path.isdir(env_path)])\n",
    "    print(\"This is loading the {}th model with the following values:\".format(model_number_to_load))\n",
    "    model_count = model_number_to_load\n",
    "\n",
    "env_summary_path = 'Projects/{}/{}/m{}/Summaries'.format(project_name, env_name, model_count)\n",
    "env_model_path = 'Projects/{}/{}/m{}/Models'.format(project_name, env_name, model_count)\n",
    "\n",
    "if not os.path.exists(env_model_path):\n",
    "    os.makedirs(env_model_path)\n",
    "\n",
    "if not os.path.exists(env_summary_path):\n",
    "    os.makedirs(env_summary_path)\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.995 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 3 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 1 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.1 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 * 4 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 5e-5 # Model learning rate.\n",
    "hidden_units = 32 # Number of units in hidden layer.\n",
    "batch_size = 64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'projet_name':project_name, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffer_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}\n",
    "\n",
    "documentation = \\\n",
    "'''\n",
    "state.Add(transform.position.x);\n",
    "        state.Add(transform.position.z);\n",
    "\n",
    "        state.Add(transform.rotation.eulerAngles.y / 180.0f - 1.0f);\n",
    "\n",
    "        state.Add(Target.transform.position.x - transform.position.x);\n",
    "        state.Add(Target.transform.position.z - transform.position.z);\n",
    "\n",
    "        Vector3[]  obs_points = FindClosestTargetPoints(\"Obstacle\", 1);\n",
    "        foreach(Vector3 obs_point in obs_points) {\n",
    "            if(DebugMode) Debug.DrawLine(transform.position, obs_point);\n",
    "            state.Add(obs_point.x - transform.position.x);\n",
    "            state.Add(obs_point.z - transform.position.z);\n",
    "        }\n",
    "\n",
    "        Vector3[]  wall_points = FindClosestTargetPoints(\"Wall\", 2);\n",
    "        foreach (Vector3 wall_point in wall_points) {\n",
    "            if (DebugMode) Debug.DrawLine(transform.position, wall_point);\n",
    "            state.Add(wall_point.x - transform.position.x);\n",
    "            state.Add(wall_point.z - transform.position.z);\n",
    "        }\n",
    "        \n",
    "    private void OnCollisionEnter(Collision collision) {\n",
    "\n",
    "        if (collision.collider.gameObject.tag == \"Target\" && hitting_target_rewards_one) {\n",
    "            reward += 1;\n",
    "            done = true;\n",
    "        }\n",
    "        action = (int)act[0];\n",
    "        if (action == 0) {\n",
    "            car_rb.AddForce(transform.forward * speed_mult, ForceMode.VelocityChange);\n",
    "        }\n",
    "        if (action == 1) {\n",
    "            car_rb.AddForce(-transform.forward * speed_mult, ForceMode.VelocityChange);\n",
    "        }\n",
    "        if (action == 2) {\n",
    "            car_rb.AddTorque(0f, turn_mult, 0f, ForceMode.VelocityChange);\n",
    "        }\n",
    "        if (action == 3) {\n",
    "            car_rb.AddTorque(0f, -turn_mult, 0f, ForceMode.VelocityChange);\n",
    "        }\n",
    "\n",
    "        if (car_rb.velocity.magnitude > maxSpeed) car_rb.velocity = car_rb.velocity * slowing_down_constant;\n",
    "       \n",
    "            case RewardStyle.SimpleProgress:\n",
    "                hitting_target_rewards_one = true;\n",
    "                float this_distance = Vector3.Distance(transform.position, Target.transform.position);\n",
    "                float delta = last_distance - this_distance;\n",
    "                reward += delta / 10f;\n",
    "                last_distance = this_distance;\n",
    "                //reward -= MinusRewardStep;\n",
    "    }\n",
    "'''\n",
    "if True:\n",
    "    pprint(hyperparameter_dict)\n",
    "    \n",
    "#Saving the model details in the summary.\n",
    "model_name = \"{}_m{}\".format(env_name, model_count)    \n",
    "with open('./Projects/{}/{}/m{}/Summaries/{}.txt'.format(project_name, env_name, model_count, model_name), 'w') as file:\n",
    "    file.write(\"gamma = {}\\n\".format(gamma))\n",
    "    file.write(\"lambd = {}\\n\".format(lambd))\n",
    "    file.write(\"time_horizon = {}\\n\".format(time_horizon))\n",
    "    file.write(\"beta = {}\\n\".format(beta))\n",
    "    file.write(\"num_epoch = {}\\n\".format(num_epoch))\n",
    "    file.write(\"epsilon = {}\\n\".format(epsilon))\n",
    "    file.write(\"buffer_size = {}\\n\".format(buffer_size))\n",
    "    file.write(\"learning_rate = {}\\n\".format(learning_rate))\n",
    "    file.write(\"hidden_units = {}\\n\".format(hidden_units))\n",
    "    file.write(\"batch_size = {}\\n\\n\".format(batch_size))\n",
    "    file.write(\"documentation = {}\\n\".format(documentation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects/Steering/Builds/v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 11\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 4\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# When the environment crashes it take a while for the socket to get freed up. This random port selection within a range\n",
    "# helps with that.\n",
    "port_save = random.randint(0,100)\n",
    "print(project_path)\n",
    "env = UnityEnvironment(file_name=project_path, curriculum=curriculum_file, base_port = 6083 + port_save)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Batu\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Batu\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated best model checkpoint!\n",
      "Step: 10000. Mean Reward: 1.0. Std of Reward: 0.0.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Training started.\")\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(env_summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.update_best(sess, saver=saver, model_path=env_model_path, steps=steps)\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=env_model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(env_model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_best_graph(trainer, env_model_path, model_name)\n",
    "export_graph(env_model_path, env_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "port_save = random.randint(0,100)\n",
    "env = UnityEnvironment(file_name = project_path, base_port = 7013 + port_save)\n",
    "print(str(env))\n",
    "brain_name = env.brain_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_best = False;\n",
    "print(\"The play session has started.\")\n",
    "with tf.Session() as sess:\n",
    "    if load_best:\n",
    "        model_path_to_load = env_model_path + \"/model_bests\"\n",
    "        if not os.path.exists(model_path_to_load):\n",
    "            print(\"Model bests does not exist!\")\n",
    "            model_path_to_load = model_path\n",
    "    else:\n",
    "        model_path_to_load = env_model_path\n",
    "    ckpt = tf.train.get_checkpoint_state(model_path_to_load)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    steps = sess.run(ppo_model.global_step)\n",
    "    info = env.reset(train_mode=False)[brain_name]\n",
    "    trainer = Trainer( ppo_model, sess, info, is_continuous, use_observations, use_states, training=False)\n",
    "    \n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=False)[brain_name]\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps = 0, normalize = False)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
