{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 1th model\n",
      "{'batch_size': 64,\n",
      " 'beta': 0.001,\n",
      " 'buffer_size': 2048,\n",
      " 'curriculum_file': 'Projects/Steering/Builds/v2.c.json',\n",
      " 'env_name': 'v2.c',\n",
      " 'epsilon': 0.2,\n",
      " 'gamma': 0.99,\n",
      " 'hidden_units': 64,\n",
      " 'lambd': 0.95,\n",
      " 'leaning_rate': 5e-05,\n",
      " 'max_steps': 50000000000.0,\n",
      " 'num_epoch': 5,\n",
      " 'projet_name': 'Steering',\n",
      " 'time_horizon': 2048}\n"
     ]
    }
   ],
   "source": [
    "#Change these two names when you work on a new project\n",
    "project_name = \"Steering\" # The sub-directory name for model and summary statistics or work on a new build \n",
    "env_name =     \"v2.c\" # Name of the training environment file.\n",
    "\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "\n",
    "model_number_to_load = 0 # which model to load\n",
    "                         #If this value is 0, load the last model in the given version.\n",
    "    \n",
    "load_best_model = False # You can either load the checkpoint with the best value or load the last checkpoint.\n",
    "\n",
    "max_steps = 5e10 # Set maximum number of steps to run environment.\n",
    "summary_freq = 2500 # Frequency at which to save training statistics.\n",
    "save_freq = 25000 # Frequency at which to save model.\n",
    "\n",
    "\n",
    "\n",
    "### Setting up the necesary directory structure and naming conventions\n",
    "project_path = \"Projects/{}/Builds/{}\".format(project_name, env_name)\n",
    "curiculum_path = \"Projects/{}/Builds/{}.json\".format(project_name, env_name)\n",
    "\n",
    "curriculum_file = curiculum_path\n",
    "\n",
    "env_path = \"Projects/{}/{}\".format(project_name, env_name)\n",
    "\n",
    "if not os.path.exists(env_path):\n",
    "    os.makedirs(env_path)\n",
    "\n",
    "if not load_model:\n",
    "    model_count = len([i for i in os.listdir(env_path) if os.path.isdir(env_path)]) + 1\n",
    "    print(\"This is the {}th model\".format(model_count))\n",
    "elif load_model:\n",
    "    if model_number_to_load == 0:\n",
    "        model_number_to_load =  len([i for i in os.listdir(env_path) if os.path.isdir(env_path)])\n",
    "    print(\"This is loading the {}th model with the following values:\".format(model_number_to_load))\n",
    "    model_count = model_number_to_load\n",
    "\n",
    "env_summary_path = 'Projects/{}/{}/m{}/Summaries'.format(project_name, env_name, model_count)\n",
    "env_model_path = 'Projects/{}/{}/m{}/Models'.format(project_name, env_name, model_count)\n",
    "\n",
    "if not os.path.exists(env_model_path):\n",
    "    os.makedirs(env_model_path)\n",
    "\n",
    "if not os.path.exists(env_summary_path):\n",
    "    os.makedirs(env_summary_path)\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 5e-5 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'projet_name':project_name, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffer_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}\n",
    "\n",
    "documentation = \\\n",
    "'''\n",
    "NUM ENV = 1\n",
    "\n",
    "STATES:\n",
    "        state.Add(transform.position.x);\n",
    "        state.Add(transform.position.z);\n",
    "\n",
    "        state.Add(transform.rotation.eulerAngles.y / 180.0f - 1.0f);\n",
    "\n",
    "        state.Add(car_rb.velocity.x);\n",
    "        state.Add(car_rb.velocity.z);\n",
    "\n",
    "        state.Add(car_rb.angularVelocity.y);\n",
    "\n",
    "\n",
    "        Vector3[] target_points = FindClosestTargetPoints(\"Target\", 1);\n",
    "        foreach (Vector3 target_point in target_points) {\n",
    "            if (DebugMode) Debug.DrawLine(transform.position, target_point);\n",
    "            state.Add(target_point.x - transform.position.x);\n",
    "            state.Add(target_point.z - transform.position.z);\n",
    "        }\n",
    "\n",
    "        Vector3[]  obs_points = FindClosestTargetPoints(\"Obstacle\", 1);\n",
    "        foreach(Vector3 obs_point in obs_points) {\n",
    "            if(DebugMode) Debug.DrawLine(transform.position, obs_point);\n",
    "            state.Add(obs_point.x - transform.position.x);\n",
    "            state.Add(obs_point.z - transform.position.z);\n",
    "        }\n",
    "\n",
    "        Vector3[]  wall_points = FindClosestTargetPoints(\"Wall\", 2);\n",
    "        foreach (Vector3 wall_point in wall_points) {\n",
    "            if (DebugMode) Debug.DrawLine(transform.position, wall_point);\n",
    "            state.Add(wall_point.x - transform.position.x);\n",
    "            state.Add(wall_point.z - transform.position.z);\n",
    "        }\n",
    "        //Debug.Log(string.Format(\"Collect state and cummulative reward is:{0}\", CumulativeReward));\n",
    "        return state;\n",
    "        \n",
    "    private void OnCollisionEnter(Collision collision) {\n",
    "\n",
    "COLLISION:\n",
    "        if (collision.collider.gameObject.tag == \"Target\" && hitting_target_rewards_one) {\n",
    "            reward += 1;\n",
    "            done = true;\n",
    "            //Debug.Log(string.Format(\"OnCollisionEnter cummulative reward is:{0}\", CumulativeReward));\n",
    "        }\n",
    "        \n",
    "AGENT STEP:\n",
    "        \n",
    "        action = (int)act[0];\n",
    "        if (action == 0) {\n",
    "            car_rb.AddForce(transform.forward * speed_mult, ForceMode.VelocityChange);\n",
    "        }\n",
    "        if (action == 1) {\n",
    "            car_rb.AddForce(-transform.forward * speed_mult, ForceMode.VelocityChange);\n",
    "        }\n",
    "        if (action == 2) {\n",
    "            car_rb.AddTorque(0f, turn_mult, 0f, ForceMode.VelocityChange);\n",
    "        }\n",
    "        if (action == 3) {\n",
    "            car_rb.AddTorque(0f, -turn_mult, 0f, ForceMode.VelocityChange);\n",
    "        }\n",
    "\n",
    "        if (car_rb.velocity.magnitude > maxSpeed) car_rb.velocity = car_rb.velocity * slowing_down_constant;\n",
    "\n",
    "            case RewardStyle.SimpleProgress:\n",
    "                hitting_target_rewards_one = true;\n",
    "\n",
    "                float this_distance = Vector3.Distance(transform.position, Target.transform.position);\n",
    "                float delta = last_distance - this_distance;\n",
    "                last_distance = this_distance;\n",
    "\n",
    "                reward += delta / 10f;\n",
    "                reward -= MinusRewardStep;\n",
    "\n",
    "                break;\n",
    "        }\n",
    "'''\n",
    "if True:\n",
    "    pprint(hyperparameter_dict)\n",
    "    \n",
    "#Saving the model details in the summary.\n",
    "model_name = \"{}_m{}\".format(env_name, model_count)    \n",
    "with open('./Projects/{}/{}/m{}/Summaries/{}.txt'.format(project_name, env_name, model_count, model_name), 'w') as file:\n",
    "    file.write(\"gamma = {}\\n\".format(gamma))\n",
    "    file.write(\"lambd = {}\\n\".format(lambd))\n",
    "    file.write(\"time_horizon = {}\\n\".format(time_horizon))\n",
    "    file.write(\"beta = {}\\n\".format(beta))\n",
    "    file.write(\"num_epoch = {}\\n\".format(num_epoch))\n",
    "    file.write(\"epsilon = {}\\n\".format(epsilon))\n",
    "    file.write(\"buffer_size = {}\\n\".format(buffer_size))\n",
    "    file.write(\"learning_rate = {}\\n\".format(learning_rate))\n",
    "    file.write(\"hidden_units = {}\\n\".format(hidden_units))\n",
    "    file.write(\"batch_size = {}\\n\\n\".format(batch_size))\n",
    "    file.write(\"documentation = {}\\n\".format(documentation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects/Steering/Builds/v2.c\n"
     ]
    },
    {
     "ename": "UnityTimeOutException",
     "evalue": "The Unity environment took too long to respond. Make sure Projects/Steering/Builds/v2.c does not need user interaction to launch and that the Academy and the external Brain(s) are attached to objects in the Scene.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file_name, worker_id, base_port, curriculum)\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36maccept\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[0mfd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;31m# If our type has the SOCK_NONBLOCK flag, we shouldn't pass it onto the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mtimeout\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnityTimeOutException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-1264e0e0ce70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mport_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurriculum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurriculum_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_port\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6083\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mport_save\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mbrain_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternal_brain_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file_name, worker_id, base_port, curriculum)\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[1;34m\"The Unity environment took too long to respond. Make sure {} does not need user interaction to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[1;34m\"launch and that the Academy and the external Brain(s) are attached to objects in the Scene.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                     .format(str(file_name)))\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"apiNumber\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnityTimeOutException\u001b[0m: The Unity environment took too long to respond. Make sure Projects/Steering/Builds/v2.c does not need user interaction to launch and that the Academy and the external Brain(s) are attached to objects in the Scene."
     ]
    }
   ],
   "source": [
    "# When the environment crashes it take a while for the socket to get freed up. This random port selection within a range\n",
    "# helps with that.\n",
    "port_save = random.randint(0,100)\n",
    "print(project_path)\n",
    "env = UnityEnvironment(file_name=project_path, curriculum=curriculum_file, base_port = 6083 + port_save)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v1.1c/m6/Models\\model-750000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v1.1c/m6/Models\\model-750000.cptk\n",
      "C:\\Users\\Batu\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Batu\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Updated best model checkpoint!\n",
      "Step: 752500. Mean Reward: 1.5291530745000004. Std of Reward: 0.00011110243021643268.\n",
      "Updated best model checkpoint!\n",
      "Step: 755000. Mean Reward: 1.5291655745000001. Std of Reward: 2.220446049250313e-16.\n",
      "Step: 757500. Mean Reward: 1.5291655745000001. Std of Reward: 2.220446049250313e-16.\n",
      "Step: 760000. Mean Reward: 1.5115054351756099. Std of Reward: 0.1588301807248924.\n",
      "Step: 762500. Mean Reward: 1.5291655745000001. Std of Reward: 2.220446049250313e-16.\n",
      "Step: 765000. Mean Reward: 1.5291655745000001. Std of Reward: 2.220446049250313e-16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 1 : \ttarget_position_z -> 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated best model checkpoint!\n",
      "Step: 767500. Mean Reward: 1.7646641824793654. Std of Reward: 0.47471826219302937.\n",
      "Updated best model checkpoint!\n",
      "Step: 770000. Mean Reward: 2.487079911400001. Std of Reward: 4.440892098500626e-16.\n",
      "Step: 772500. Mean Reward: 2.487079911400001. Std of Reward: 4.440892098500626e-16.\n",
      "Step: 775000. Mean Reward: 2.4236425330461544. Std of Reward: 0.3908920739499417.\n",
      "Saved Model\n",
      "Step: 777500. Mean Reward: 2.487079911400001. Std of Reward: 4.440892098500626e-16.\n",
      "Step: 780000. Mean Reward: 2.487079911400001. Std of Reward: 4.440892098500626e-16.\n",
      "Step: 782500. Mean Reward: 2.487079911400001. Std of Reward: 4.440892098500626e-16.\n",
      "Step: 785000. Mean Reward: 2.425228467505. Std of Reward: 0.38610204671325815.\n",
      "Step: 787500. Mean Reward: 2.487079911400001. Std of Reward: 4.440892098500626e-16.\n",
      "Step: 790000. Mean Reward: 2.487079911400001. Std of Reward: 4.440892098500626e-16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 2 : \ttarget_position_z -> 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated best model checkpoint!\n",
      "Step: 792500. Mean Reward: 2.884770702583873. Std of Reward: 0.7143390986801341.\n",
      "Updated best model checkpoint!\n",
      "Step: 795000. Mean Reward: 3.473911396900001. Std of Reward: 4.440892098500626e-16.\n",
      "Step: 797500. Mean Reward: 3.4736445214240006. Std of Reward: 0.0013074174821248394.\n",
      "Step: 800000. Mean Reward: 3.392930521676924. Std of Reward: 0.4047044217327254.\n",
      "Saved Model\n",
      "Step: 802500. Mean Reward: 3.4723407407772813. Std of Reward: 0.007694612124084712.\n",
      "Step: 805000. Mean Reward: 3.473911396900001. Std of Reward: 4.440892098500626e-16.\n",
      "Step: 807500. Mean Reward: 3.3912925769961544. Std of Reward: 0.4102449063290584.\n",
      "Step: 810000. Mean Reward: 3.473911396900001. Std of Reward: 4.440892098500626e-16.\n",
      "Step: 812500. Mean Reward: 3.473911396900001. Std of Reward: 4.440892098500626e-16.\n",
      "Step: 815000. Mean Reward: 3.473911396900001. Std of Reward: 4.440892098500626e-16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 3 : \ttarget_position_z -> 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated best model checkpoint!\n",
      "Step: 817500. Mean Reward: 4.017759826042857. Std of Reward: 0.733045613536915.\n",
      "Updated best model checkpoint!\n",
      "Step: 820000. Mean Reward: 4.43816258485. Std of Reward: 0.0061643250567661595.\n",
      "Updated best model checkpoint!\n",
      "Step: 822500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 825000. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Saved Model\n",
      "Step: 827500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 830000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 832500. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 835000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 837500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 840000. Mean Reward: 4.438600670310525. Std of Reward: 0.0062639377619719255.\n",
      "Step: 842500. Mean Reward: 4.229264954647618. Std of Reward: 0.9425569680312351.\n",
      "Step: 845000. Mean Reward: 4.4393208908999995. Std of Reward: 0.003208300585370165.\n",
      "Step: 847500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 850000. Mean Reward: 4.2180576117249995. Std of Reward: 0.9644762340836116.\n",
      "Saved Model\n",
      "Step: 852500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 855000. Mean Reward: 4.439537460326315. Std of Reward: 0.0022894743256965653.\n",
      "Step: 857500. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 860000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 862500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 865000. Mean Reward: 4.229264954647618. Std of Reward: 0.9425569680312352.\n",
      "Step: 867500. Mean Reward: 4.4388643382894735. Std of Reward: 0.005145289266599561.\n",
      "Step: 870000. Mean Reward: 4.4398926791105255. Std of Reward: 0.0007824086589759912.\n",
      "Step: 872500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 875000. Mean Reward: 4.217929244679999. Std of Reward: 0.9644486210798857.\n",
      "Saved Model\n",
      "Step: 877500. Mean Reward: 4.439358701085. Std of Reward: 0.0031314047335799897.\n",
      "Step: 880000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 882500. Mean Reward: 4.218006786169999. Std of Reward: 0.9658861180801627.\n",
      "Step: 885000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 887500. Mean Reward: 4.439901845884999. Std of Reward: 0.0007638914386704009.\n",
      "Step: 890000. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 892500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 895000. Mean Reward: 4.438974294273684. Std of Reward: 0.0042740052627837245.\n",
      "Step: 897500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 900000. Mean Reward: 4.229264954647618. Std of Reward: 0.9425569680312351.\n",
      "Saved Model\n",
      "Step: 902500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 905000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 907500. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 910000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 912500. Mean Reward: 4.438540354926315. Std of Reward: 0.00543346936659134.\n",
      "Step: 915000. Mean Reward: 4.218634487649999. Std of Reward: 0.9646043227540992.\n",
      "Step: 917500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 920000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 922500. Mean Reward: 4.228810560685714. Std of Reward: 0.942457523676641.\n",
      "Step: 925000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 927500. Mean Reward: 4.4397264247210515. Std of Reward: 0.0014877662961693453.\n",
      "Step: 930000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 932500. Mean Reward: 4.229264954647618. Std of Reward: 0.9425569680312351.\n",
      "Step: 935000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 937500. Mean Reward: 4.4397264247210515. Std of Reward: 0.0014877662961693453.\n",
      "Step: 940000. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 942500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 945000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 947500. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 950000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 952500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 955000. Mean Reward: 4.218774347649999. Std of Reward: 0.9646363098830039.\n",
      "Step: 957500. Mean Reward: 4.4400270946. Std of Reward: 0.00021794494717691286.\n",
      "Step: 960000. Mean Reward: 4.4399825051263155. Std of Reward: 0.00040130914962571864.\n",
      "Step: 962500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 965000. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 967500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 970000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 972500. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 975000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 977500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 980000. Mean Reward: 4.229264954647618. Std of Reward: 0.9425569680312352.\n",
      "Step: 982500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 985000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 987500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 990000. Mean Reward: 4.229264954647618. Std of Reward: 0.9425569680312351.\n",
      "Step: 992500. Mean Reward: 4.436494861347367. Std of Reward: 0.010365303712015021.\n",
      "Step: 995000. Mean Reward: 4.438804832305262. Std of Reward: 0.004342151006510296.\n",
      "Step: 997500. Mean Reward: 4.440024463021052. Std of Reward: 0.0002232968782693123.\n",
      "Step: 1000000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1002500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1005000. Mean Reward: 4.217952852354999. Std of Reward: 0.9644536750198028.\n",
      "Updated best model checkpoint!\n",
      "Step: 1007500. Mean Reward: 4.440107618726315. Std of Reward: 0.0001295029002462168.\n",
      "Step: 1010000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1012500. Mean Reward: 4.228624845071428. Std of Reward: 0.9447876510513277.\n",
      "Step: 1015000. Mean Reward: 4.4399906091. Std of Reward: 0.00036692690114555396.\n",
      "Step: 1017500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1020000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1022500. Mean Reward: 4.218413935065. Std of Reward: 0.9659779207035054.\n",
      "Step: 1025000. Mean Reward: 4.43983863206. Std of Reward: 0.001039434113680198.\n",
      "Saved Model\n",
      "Step: 1027500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1030000. Mean Reward: 4.218413935065. Std of Reward: 0.9659779207035054.\n",
      "Step: 1032500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1035000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1037500. Mean Reward: 4.2169488729. Std of Reward: 0.9642384503998549.\n",
      "Step: 1040000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1042500. Mean Reward: 4.439254581505263. Std of Reward: 0.0034896275214189396.\n",
      "Step: 1045000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1047500. Mean Reward: 4.229264954647618. Std of Reward: 0.9425569680312351.\n",
      "Step: 1050000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1052500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1055000. Mean Reward: 4.217839062495. Std of Reward: 0.9644294156829335.\n",
      "Step: 1057500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1060000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1062500. Mean Reward: 4.21767668734. Std of Reward: 0.9643944471680606.\n",
      "Step: 1065000. Mean Reward: 4.439636707873683. Std of Reward: 0.0018684026431344677.\n",
      "Step: 1067500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1070000. Mean Reward: 4.228969323614285. Std of Reward: 0.9438790701712055.\n",
      "Step: 1072500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1075000. Mean Reward: 4.4388228983526306. Std of Reward: 0.005321104028717784.\n",
      "Saved Model\n",
      "Step: 1077500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1080000. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 1082500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1085000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1087500. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 1090000. Mean Reward: 4.439725871315789. Std of Reward: 0.0014901141958553882.\n",
      "Step: 1092500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1095000. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 1097500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1100000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1102500. Mean Reward: 4.218518125965. Std of Reward: 0.9645781733798182.\n",
      "Step: 1105000. Mean Reward: 4.4400270946. Std of Reward: 0.00021794494717691286.\n",
      "Step: 1107500. Mean Reward: 4.439375267310526. Std of Reward: 0.0029776010136517736.\n",
      "Step: 1110000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1112500. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 1115000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1117500. Mean Reward: 4.438500471963684. Std of Reward: 0.006689043345066348.\n",
      "Step: 1120000. Mean Reward: 4.218413935065. Std of Reward: 0.9659779207035054.\n",
      "Step: 1122500. Mean Reward: 4.439264994289473. Std of Reward: 0.0034454498194612616.\n",
      "Step: 1125000. Mean Reward: 4.437869147947368. Std of Reward: 0.006467640010452583.\n",
      "Saved Model\n",
      "Step: 1127500. Mean Reward: 4.228725095195237. Std of Reward: 0.9438250764963599.\n",
      "Step: 1130000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1132500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1135000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1137500. Mean Reward: 4.215866288555. Std of Reward: 0.964016094442949.\n",
      "Step: 1140000. Mean Reward: 4.43872634963. Std of Reward: 0.005887760822725712.\n",
      "Step: 1142500. Mean Reward: 4.439074768978947. Std of Reward: 0.004252507461620095.\n",
      "Step: 1145000. Mean Reward: 4.440024463021052. Std of Reward: 0.0002232968782693123.\n",
      "Step: 1147500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1150000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1152500. Mean Reward: 4.217277471045. Std of Reward: 0.9657298060714664.\n",
      "Step: 1155000. Mean Reward: 4.439379835626315. Std of Reward: 0.002958219291211371.\n",
      "Step: 1157500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1160000. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490787.\n",
      "Step: 1162500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1165000. Mean Reward: 4.438841742636842. Std of Reward: 0.005241154501806131.\n",
      "Step: 1167500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1170000. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 1172500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1175000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1177500. Mean Reward: 4.218724347649999. Std of Reward: 0.9646248636490786.\n",
      "Step: 1180000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1182500. Mean Reward: 4.439093890252631. Std of Reward: 0.0041713827678978264.\n",
      "Step: 1185000. Mean Reward: 4.229264954647618. Std of Reward: 0.9425569680312351.\n",
      "Step: 1187500. Mean Reward: 4.438929686415789. Std of Reward: 0.004116961320369487.\n",
      "Step: 1190000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1192500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1195000. Mean Reward: 4.229264954647618. Std of Reward: 0.9425569680312351.\n",
      "Step: 1197500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1200000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1202500. Mean Reward: 4.217503134289999. Std of Reward: 0.9643592824179429.\n",
      "Step: 1205000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1207500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1210000. Mean Reward: 4.216868669119999. Std of Reward: 0.9666515684172035.\n",
      "Step: 1212500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1215000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1217500. Mean Reward: 4.2167311417999995. Std of Reward: 0.966623767590371.\n",
      "Step: 1220000. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n",
      "Step: 1222500. Mean Reward: 4.440077094599999. Std of Reward: 0.0.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-ecb8211639cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Decide and take an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value_estimate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Training started.\")\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(env_model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(env_summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.update_best(sess, saver=saver, model_path=env_model_path, steps=steps)\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=env_model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(env_model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input checkpoint 'Projects/Steering/v1.1c/m6/Models/model_bests\\best_model.cptk' doesn't exist!\n",
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v1.1c/m6/Models\\model-1200000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v1.1c/m6/Models\\model-1200000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_best_graph(trainer, env_model_path, model_name)\n",
    "export_graph(env_model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\ttarget_position_z -> 0.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 14\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 4\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: Forward, Backwards, Right, Left\n"
     ]
    }
   ],
   "source": [
    "port_save = random.randint(0,100)\n",
    "env = UnityEnvironment(file_name = project_path, base_port = 7013 + port_save)\n",
    "print(str(env))\n",
    "brain_name = env.brain_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The play session has started.\n",
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v1.1m/m1/Models/model_bests\\best_model.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Projects/Steering/v1.1m/m1/Models/model_bests\\best_model.cptk\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "unpack requires a buffer of 4 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-67cf656680fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_done\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, memory, value)\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"STEP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_brains\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"brain_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mn_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"agents\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"RECEIVED\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UnityML\\BatuForkV2\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmessage_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmessage_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: unpack requires a buffer of 4 bytes"
     ]
    }
   ],
   "source": [
    "load_best = True;\n",
    "print(\"The play session has started.\")\n",
    "with tf.Session() as sess:\n",
    "    if load_best:\n",
    "        model_path_to_load = env_model_path + \"/model_bests\"\n",
    "        if not os.path.exists(model_path_to_load):\n",
    "            print(\"Model bests does not exist!\")\n",
    "            model_path_to_load = model_path\n",
    "    else:\n",
    "        model_path_to_load = env_model_path\n",
    "    ckpt = tf.train.get_checkpoint_state(model_path_to_load)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    steps = sess.run(ppo_model.global_step)\n",
    "    info = env.reset(train_mode=False)[brain_name]\n",
    "    trainer = Trainer( ppo_model, sess, info, is_continuous, use_observations, use_states, training=False)\n",
    "    \n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=False)[brain_name]\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps = 0, normalize = False)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
